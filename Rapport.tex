\documentclass[11pt,a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage[french]{babel}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{geometry}
\usepackage{titlesec}
\usepackage{fancyhdr}
\usepackage{float}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usepackage{enumitem}

% Configuration de la page
\geometry{
    left=2.5cm,
    right=2.5cm,
    top=3cm,
    bottom=3cm,
    headheight=15pt
}

% Configuration des hyperliens
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    filecolor=blue,
    urlcolor=blue,
    citecolor=blue,
    pdftitle={HealthFlow-MS - Rapport de Projet},
    pdfauthor={Équipe HealthFlow},
    pdfsubject={Système de Prédiction de Réadmission Hospitalière},
    pdfkeywords={FHIR, Microservices, Machine Learning, XGBoost, NLP}
}

% Configuration des listes - Utiliser des points au lieu de tirets
% Forcer les points même avec babel french
\frenchbsetup{StandardLists=true}
\setlist[itemize]{label=$\bullet$}
\setlist[itemize,2]{label=$\cdot$}
\setlist[itemize,3]{label=$\diamond$}
\setlist[itemize,4]{label=$\ast$}
% Redéfinir aussi les commandes LaTeX standard
\renewcommand{\labelitemi}{$\bullet$}
\renewcommand{\labelitemii}{$\cdot$}
\renewcommand{\labelitemiii}{$\diamond$}
\renewcommand{\labelitemiv}{$\ast$}

% Configuration du code
\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue}\bfseries,
    commentstyle=\color{green!60!black},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    stepnumber=1,
    numbersep=5pt,
    backgroundcolor=\color{gray!10},
    frame=single,
    breaklines=true,
    breakatwhitespace=true,
    tabsize=2,
    showspaces=false,
    showstringspaces=false,
    showtabs=false
}

% Configuration des titres avec tailles différentes
\titleformat{\chapter}[display]
{\normalfont\huge\bfseries}{\chaptertitlename\ \thechapter}{20pt}{\huge}
\titlespacing*{\chapter}{0pt}{30pt}{25pt}

\titleformat{\section}
{\normalfont\Large\bfseries}{\thesection}{1em}{}
\titlespacing*{\section}{0pt}{12pt}{6pt}

\titleformat{\subsection}
{\normalfont\large\bfseries}{\thesubsection}{1em}{}
\titlespacing*{\subsection}{0pt}{10pt}{5pt}

\titleformat{\subsubsection}
{\normalfont\normalsize\bfseries}{\thesubsubsection}{1em}{}
\titlespacing*{\subsubsection}{0pt}{8pt}{4pt}

% Espacement des paragraphes
\setlength{\parindent}{1.5em}
\setlength{\parskip}{0.5em plus 0.1em minus 0.1em}
\linespread{1.1}

% En-têtes et pieds de page
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\leftmark}
\fancyhead[R]{\thepage}
\fancyfoot[C]{HealthFlow-MS - Rapport de Projet}

\begin{document}

% Page de titre personnalisée
\begin{titlepage}
    \centering
    
    \vspace*{1cm}
    
    % Titre principal
    {\Huge\textbf{HealthFlow-MS}}\\[0.4cm]
    {\Large\textbf{Système de Prédiction de Réadmission Hospitalière}}\\[0.2cm]
    {\large basé sur l'Analyse de Données FHIR}\\[0.25cm]
    {\normalsize Architecture Microservices avec Intelligence Artificielle}\\[1cm]
    
    % Ligne de séparation décorative
    \rule{0.75\textwidth}{1pt}\\[0.6cm]
    
    % Institution
    {\large\textbf{Groupe 8 - EMSI}}\\[0.15cm]
    {\normalsize École Marocaine des Sciences de l'Ingénieur}\\[0.1cm]
    {\normalsize Marrakech, Maroc}\\[0.8cm]
    
    % Repository GitHub
    \rule{0.5\textwidth}{0.5pt}\\[0.4cm]
    {\normalsize\textbf{Repository GitHub}}\\[0.2cm]
    {\small\url{https://github.com/OsamaMansouri/HealthFlowMS}}\\[0.4cm]
    \rule{0.5\textwidth}{0.5pt}\\[0.6cm]
    
    % Section Équipe - 3 colonnes
    {\normalsize\textbf{ÉQUIPE DE DÉVELOPPEMENT}}\\[0.4cm]
    \rule{0.6\textwidth}{0.5pt}\\[0.5cm]
    
    \begin{minipage}{0.95\textwidth}
        \centering
        \begin{minipage}{0.32\textwidth}
            \centering
            \textbf{Osama MANSOURI}\\[0.2cm]
            \texttt{\small mansouri.osama@gmail.com}
        \end{minipage}
        \hfill
        \begin{minipage}{0.32\textwidth}
            \centering
            \textbf{Sohaib LAARICHI}\\[0.2cm]
            \texttt{\small Sohaiblaatichi112@gmail.com}
        \end{minipage}
        \hfill
        \begin{minipage}{0.32\textwidth}
            \centering
            \textbf{Abouelkemhe Salah Eddine}\\[0.2cm]
            \texttt{\small slaaaheddine@gmail.com}
        \end{minipage}
    \end{minipage}\\[1cm]
    
    % Section Encadrement - 3 colonnes
    {\normalsize\textbf{ENCADRÉ PAR}}\\[0.4cm]
    \rule{0.6\textwidth}{0.5pt}\\[0.5cm]
    
    \begin{minipage}{0.95\textwidth}
        \centering
        \begin{minipage}{0.32\textwidth}
            \centering
            \textbf{Pr. Mohamed LACHGAR}\\[0.15cm]
            {\small\textit{Microservices}}
        \end{minipage}
        \hfill
        \begin{minipage}{0.32\textwidth}
            \centering
            \textbf{Pr. Oumayma OUEDRHIRI}\\[0.15cm]
            {\small\textit{Machine Learning}}
        \end{minipage}
        \hfill
        \begin{minipage}{0.32\textwidth}
            \centering
            \textbf{Pr. Hiba TABBAA}\\[0.15cm]
            {\small\textit{Data Mining \& NLP}}
        \end{minipage}
    \end{minipage}\\[1cm]
    
    % Date
    \rule{0.75\textwidth}{1pt}\\[0.5cm]
    {\normalsize\today}
    
    \vfill
\end{titlepage}
\newpage

% ============================================
% RÉSUMÉ / ABSTRACT
% ============================================
\begin{abstract}
Ce document présente \textbf{HealthFlow-MS}, un système complet de prédiction de réadmission hospitalière basé sur une architecture microservices moderne. Le système exploite les standards FHIR (Fast Healthcare Interoperability Resources) pour l'intégration de données médicales, combine des techniques avancées de Machine Learning (XGBoost) et de traitement du langage naturel (BioBERT, spaCy) pour analyser les données cliniques, et fournit des prédictions de risque avec explicabilité via SHAP. L'architecture modulaire comprend 8 microservices indépendants qui assurent l'interopérabilité, la confidentialité (conformité HIPAA Safe Harbor), et la transparence des prédictions. Le modèle de prédiction XGBoost atteint une performance de 0.82 en AUC-ROC, tandis que l'analyse NLP des notes cliniques permet d'extraire plus de 30 caractéristiques pertinentes pour améliorer la précision des prédictions. Le système fournit également un dashboard d'audit et de fairness pour détecter et corriger les biais potentiels dans les prédictions, garantissant ainsi l'équité et la transparence dans l'utilisation de l'intelligence artificielle en santé.
\end{abstract}

\textbf{Mots-clés:} FHIR, Microservices, Machine Learning, XGBoost, NLP, BioBERT, Prédiction de Réadmission, SHAP, HIPAA, Architecture Distribuée

\newpage

% Table des matières
\tableofcontents
\newpage

% Liste des figures
\listoffigures
\newpage

% Liste des tableaux
\listoftables
\newpage

% ============================================
% INTRODUCTION
% ============================================
\chapter{Introduction}

\section{Contexte et Motivation}

La réadmission hospitalière est un indicateur critique de la qualité des soins et représente un défi majeur pour les systèmes de santé modernes. Les réadmissions non planifiées dans les 30 jours suivant une sortie d'hôpital sont non seulement coûteuses, mais peuvent également indiquer des problèmes dans la continuité des soins ou dans la gestion des patients à risque.

\textbf{HealthFlow-MS} a été conçu pour répondre à ce défi en fournissant une solution complète et innovante qui transforme la manière dont les établissements de santé anticipent et gèrent les risques de réadmission. Le système offre une prédiction précise du risque de réadmission à 30 jours en exploitant les dernières avancées en intelligence artificielle et en traitement du langage naturel. Contrairement aux systèmes traditionnels qui se basent uniquement sur des règles statiques ou des scores simples, HealthFlow-MS utilise un modèle XGBoost sophistiqué qui analyse plus de 30 caractéristiques cliniques différentes, incluant des données démographiques, des signes vitaux, des résultats de laboratoire, et même l'analyse sémantique des notes cliniques rédigées par les professionnels de santé.

L'une des innovations majeures du système réside dans sa capacité à fournir une explication transparente et interprétable des facteurs de risque identifiés. Grâce à l'intégration de SHAP (SHapley Additive exPlanations), les cliniciens peuvent comprendre non seulement \textit{quel} est le niveau de risque d'un patient, mais également \textit{pourquoi} ce niveau de risque a été calculé. Cette transparence est cruciale dans le domaine médical, où la confiance dans les systèmes d'aide à la décision ne peut pas être basée uniquement sur des métriques statistiques, mais doit reposer sur une compréhension profonde des mécanismes de prédiction.

L'intégration fluide avec les systèmes d'information hospitaliers existants est garantie par l'utilisation du standard FHIR (Fast Healthcare Interoperability Resources), qui constitue la norme internationale pour l'échange de données médicales. Cette approche permet à HealthFlow-MS de s'intégrer naturellement dans l'écosystème informatique hospitalier sans nécessiter de modifications majeures des systèmes existants. Les données médicales sont récupérées automatiquement depuis les systèmes source, transformées selon le standard FHIR, puis traitées par le pipeline d'analyse du système.

Enfin, la conformité aux normes de confidentialité les plus strictes, notamment HIPAA (Health Insurance Portability and Accountability Act) aux États-Unis et RGPD (Règlement Général sur la Protection des Données) en Europe, est assurée par un processus d'anonymisation sophistiqué qui transforme les données identifiables en données pseudonymisées. Ce processus respecte le standard HIPAA Safe Harbor, garantissant que toutes les informations permettant d'identifier directement un patient sont supprimées ou transformées avant que les données ne soient utilisées pour l'analyse et la prédiction.

\section{Objectifs du Projet}

Les objectifs de ce projet sont ambitieux et multidimensionnels, couvrant à la fois les aspects techniques, éthiques et pratiques de l'application de l'intelligence artificielle en santé. Le premier objectif, l'\textbf{interopérabilité}, vise à résoudre l'un des défis majeurs de l'informatique médicale moderne : la fragmentation des systèmes d'information. En adoptant le standard FHIR (Fast Healthcare Interoperability Resources), HealthFlow-MS peut intégrer des données médicales provenant de sources diverses et hétérogènes, transformant ainsi un paysage informatique fragmenté en un écosystème unifié où l'information circule librement et de manière standardisée entre les différents composants du système de santé.

Le deuxième objectif, la \textbf{confidentialité}, est fondamental dans le domaine médical où la protection des données personnelles n'est pas seulement une exigence légale mais une obligation éthique absolue. L'anonymisation selon les normes HIPAA Safe Harbor garantit que les données utilisées pour l'analyse et la prédiction ne peuvent pas être utilisées pour ré-identifier les patients, même en combinant plusieurs sources d'information. Ce processus d'anonymisation est sophistiqué et va bien au-delà d'une simple suppression de noms : il transforme les dates en groupes d'âge, généralise les localisations géographiques, et remplace les identifiants uniques par des pseudonymes déterministes qui préservent la cohérence des données tout en masquant l'identité réelle.

L'objectif d'\textbf{intelligence} représente le cœur technique du système. Extraire plus de 30 caractéristiques pertinentes des données cliniques nécessite une compréhension profonde à la fois du domaine médical et des techniques d'ingénierie des features. Le système combine des données structurées (signes vitaux, résultats de laboratoire, diagnostics) avec des données non structurées (notes cliniques rédigées en langage naturel), utilisant des techniques avancées de traitement du langage naturel pour transformer le texte libre en caractéristiques quantitatives exploitables par les modèles de Machine Learning.

La \textbf{prédiction} du risque de réadmission constitue l'objectif fonctionnel principal. Le modèle XGBoost utilisé atteint une performance remarquable avec un AUC-ROC de 0.82, ce qui signifie que le système peut distinguer efficacement les patients à haut risque de ceux à faible risque. Cette performance n'est pas le fruit du hasard mais résulte d'un processus rigoureux d'entraînement, de validation croisée, et d'optimisation des hyperparamètres qui garantit à la fois la précision et la généralisation du modèle à de nouveaux patients.

L'\textbf{explicabilité} via SHAP répond à une exigence critique en médecine : la nécessité de comprendre les décisions prises par les systèmes d'intelligence artificielle. Contrairement aux modèles "boîte noire" qui produisent des prédictions sans justification, HealthFlow-MS fournit des explications détaillées qui identifient les facteurs contribuant le plus au score de risque calculé, permettant ainsi aux cliniciens de prendre des décisions éclairées basées sur une compréhension complète du raisonnement du modèle.

La détection et la correction des \textbf{biais} dans les prédictions (fairness) est un objectif éthique fondamental qui garantit que le système ne perpétue pas ou n'amplifie pas les inégalités existantes dans les soins de santé. Le système calcule plusieurs métriques de fairness (Demographic Parity, Equalized Odds, Equal Opportunity) et génère des alertes lorsque des biais sont détectés, permettant ainsi une intervention proactive pour corriger les déséquilibres.

Enfin, l'\textbf{accessibilité} via une interface utilisateur intuitive garantit que les bénéfices du système ne sont pas réservés aux experts techniques mais sont accessibles à tous les professionnels de santé, des médecins aux infirmières, en passant par les gestionnaires de soins. L'interface web moderne, développée avec React et TypeScript, offre une expérience utilisateur fluide et intuitive qui masque la complexité technique sous-jacente, permettant ainsi une adoption large et rapide du système.

\section{Structure du Document}

Ce rapport présente une exploration complète et approfondie du système HealthFlow-MS, organisée de manière logique et progressive pour guider le lecteur depuis les concepts fondamentaux jusqu'aux détails d'implémentation les plus techniques. Le \textbf{Chapitre 2} introduit l'architecture globale du système, décrivant la vision d'ensemble de l'architecture microservices, les principes de conception qui ont guidé le développement, et la manière dont les différents composants interagissent pour former un système cohérent et performant. Ce chapitre établit les fondations conceptuelles nécessaires pour comprendre les choix d'architecture et les compromis techniques qui ont été faits.

Le \textbf{Chapitre 3} plonge dans les détails de chaque microservice, offrant une description exhaustive de chacun des 8 services qui composent le système. Pour chaque service, le chapitre présente son rôle spécifique dans l'écosystème global, les technologies utilisées, les fonctionnalités principales, et les endpoints exposés. Cette description détaillée permet de comprendre non seulement ce que fait chaque service, mais également pourquoi il a été conçu de cette manière et comment il contribue aux objectifs globaux du système.

Le \textbf{Chapitre 4} présente les diagrammes BPMN (Business Process Model and Notation) qui modélisent les processus métier du système. Ces diagrammes offrent une vue visuelle claire des flux de données et des interactions entre les différents services, facilitant ainsi la compréhension des workflows complexes qui sous-tendent le fonctionnement du système. Chaque diagramme est accompagné d'une description détaillée qui explique les étapes du processus, les décisions prises à chaque point, et les résultats attendus.

Le \textbf{Chapitre 5} explore en profondeur les technologies et dépendances utilisées dans le projet. Ce chapitre ne se contente pas de lister les technologies, mais explique les raisons derrière chaque choix technologique, les alternatives considérées, et les compromis acceptés. Cette analyse permet de comprendre l'écosystème technique complet du projet et les relations entre les différentes technologies utilisées.

Le \textbf{Chapitre 6} détaille la base de données et le schéma relationnel qui stocke toutes les données du système. Le chapitre présente la structure complète de la base de données, les relations entre les tables, les index créés pour optimiser les performances, et les contraintes d'intégrité qui garantissent la cohérence des données. Cette description est essentielle pour comprendre comment les données sont organisées et comment elles circulent à travers le système.

Le \textbf{Chapitre 7} constitue le cœur technique du rapport, présentant en détail les aspects d'intelligence artificielle et de Machine Learning du système. Ce chapitre explore le modèle XGBoost utilisé pour la prédiction, les techniques de traitement du langage naturel (BioBERT, spaCy) utilisées pour analyser les notes cliniques, et les méthodes d'explicabilité (SHAP) qui rendent les prédictions compréhensibles. Chaque aspect est présenté avec suffisamment de détails techniques pour permettre une compréhension approfondie tout en restant accessible.

Le \textbf{Chapitre 8} aborde les aspects critiques de sécurité et de conformité. Dans le domaine médical, la sécurité n'est pas optionnelle mais absolument essentielle. Ce chapitre détaille les mécanismes d'authentification et d'autorisation, les processus d'anonymisation des données, et la conformité aux normes réglementaires (HIPAA, RGPD). Cette présentation démontre que le système a été conçu dès le départ avec la sécurité comme priorité absolue.

Le \textbf{Chapitre 9} présente la stratégie de tests et de validation mise en place pour garantir la qualité et la fiabilité du système. Ce chapitre décrit les différents types de tests (unitaires, d'intégration, end-to-end, de performance) et explique comment ces tests contribuent à garantir que le système fonctionne correctement dans toutes les conditions. La validation du modèle ML est également présentée en détail, montrant comment le modèle a été évalué et validé avant son déploiement.

Le \textbf{Chapitre 10} fournit un guide complet de déploiement et d'installation, permettant à quiconque de déployer le système dans son propre environnement. Ce chapitre couvre tous les aspects pratiques, depuis les prérequis système jusqu'aux étapes détaillées d'installation, en passant par la configuration et la vérification du déploiement. Ce guide pratique est essentiel pour permettre l'adoption et le déploiement du système dans différents environnements.

Le \textbf{Chapitre 11} présente les résultats et performances du système, offrant une évaluation quantitative complète de la performance du modèle ML, des temps de réponse du système, et de l'utilisation des ressources. Ces résultats démontrent que le système atteint ses objectifs de performance et peut être déployé dans des environnements de production avec confiance.

Enfin, le \textbf{Chapitre 12} conclut le rapport en présentant les réalisations du projet, les points forts identifiés, les limitations actuelles, et les perspectives d'amélioration future. Ce chapitre offre une réflexion sur le travail accompli et ouvre la voie vers des développements futurs qui pourraient encore améliorer le système.

\newpage

% ============================================
% ARCHITECTURE GLOBALE
% ============================================
\chapter{Architecture Globale}

\section{Vue d'Ensemble}

Cette section présente l'architecture globale du système HealthFlow-MS, en détaillant les principes de conception et l'organisation des microservices.

HealthFlow-MS suit une architecture microservices modulaire où chaque service a une responsabilité unique et bien définie. Cette approche architecturale moderne offre plusieurs avantages significatifs par rapport aux architectures monolithiques traditionnelles.

La modularité permet à chaque service d'être développé, testé et déployé indépendamment, facilitant ainsi la maintenance et l'évolution du système. La scalabilité horizontale est rendue possible car les services peuvent être mis à l'échelle individuellement selon la charge de travail spécifique qu'ils doivent traiter. L'isolation des pannes garantit qu'une défaillance dans un service n'affecte pas les autres composants du système, améliorant ainsi la résilience globale. Enfin, cette architecture permet un choix technologique optimal pour chaque service, sélectionnant la technologie la plus adaptée à sa tâche spécifique sans être contraint par les choix des autres services.

\section{Architecture des Microservices}

Le système est composé de \textbf{8 microservices} principaux:

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Service} & \textbf{Port} & \textbf{Technologie} & \textbf{Rôle} \\
\hline
HAPI FHIR & 8090 & HAPI FHIR Server & Serveur FHIR de référence \\
ProxyFHIR & 8081 & Java/Spring Boot & Synchronisation FHIR \\
DeID & 8082 & Python/FastAPI & Anonymisation des données \\
Featurizer & 8083 & Python/FastAPI & Extraction de features + NLP \\
ModelRisque & 8084 & Python/FastAPI & Prédiction ML (XGBoost) \\
ScoreAPI & 8085 & Python/FastAPI & API principale REST \\
AuditFairness & 8086 & Python/Dash & Dashboard audit \\
Frontend & 8087 & React/TypeScript & Interface utilisateur \\
PostgreSQL & 5433 & PostgreSQL 15 & Base de données centrale \\
\hline
\end{tabular}
\caption{Liste des microservices}
\end{table}

\section{Diagramme d'Architecture Global}

La figure \ref{fig:architecture-globale} présente l'architecture complète du système avec tous les microservices et leurs interactions.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{images/bpmn/architecture-globale.png}
\caption{Architecture globale du système HealthFlow-MS}
\label{fig:architecture-globale}
\end{figure}

\section{Flux de Données Principal}

Cette section décrit le flux de données complet depuis l'ingestion jusqu'à la visualisation, en expliquant comment chaque étape transforme les données pour aboutir à une prédiction de risque.

Le flux de données principal suit une séquence logique et séquentielle qui transforme progressivement les données médicales brutes en prédictions exploitables. La première étape consiste en l'ingestion des données FHIR depuis le serveur HAPI FHIR vers la base de données PostgreSQL via le service ProxyFHIR, qui synchronise périodiquement toutes les ressources FHIR (patients, hospitalisations, observations, diagnostics).

Une fois les données synchronisées, le service DeID procède à l'anonymisation selon la méthode HIPAA Safe Harbor, supprimant ou modifiant les 18 identifiants personnels spécifiés par la réglementation. Cette étape est cruciale pour garantir la confidentialité des données avant leur traitement par les services d'analyse.

Le service Featurizer extrait ensuite plus de 30 caractéristiques pertinentes des données anonymisées, combinant des données démographiques, cliniques, de laboratoire et d'analyse NLP des notes cliniques. Ces features sont préparées pour alimenter le modèle de Machine Learning.

Le service ModelRisque utilise ensuite ces features pour calculer le score de risque de réadmission à 30 jours avec le modèle XGBoost. En parallèle, SHAP génère des explications détaillées qui identifient les facteurs contribuant le plus au score de risque calculé.

ScoreAPI expose ces résultats via une API REST sécurisée avec authentification JWT, permettant aux applications clientes d'accéder aux prédictions et explications. Enfin, le Frontend et le service AuditFairness présentent ces données aux utilisateurs sous forme de tableaux de bord interactifs et de visualisations graphiques.

\newpage

% ============================================
% MICROSERVICES DÉTAILLÉS
% ============================================
\chapter{Description des Microservices}

\section{HAPI FHIR Server}

\subsection{Présentation}

Cette section introduit le serveur HAPI FHIR, qui constitue la source de données médicales pour l'ensemble du système HealthFlow-MS.

HAPI FHIR est un serveur FHIR de référence open-source développé par l'Université de la Colombie-Britannique. Il sert de source de données médicales pour le système HealthFlow-MS, stockant toutes les ressources FHIR selon le standard HL7 FHIR R4. Le serveur gère les ressources principales telles que Patient (informations démographiques), Encounter (hospitalisations), Observation (signes vitaux et résultats de laboratoire), et Condition (diagnostics et maladies). Cette conformité au standard FHIR garantit l'interopérabilité avec d'autres systèmes d'information hospitaliers et facilite l'intégration future avec d'autres plateformes de santé.

\subsection{Technologies}

Le serveur HAPI FHIR est déployé via une image Docker officielle (\texttt{hapiproject/hapi:latest}) qui encapsule toutes les dépendances nécessaires. Il implémente le standard HL7 FHIR R4, garantissant la compatibilité avec les systèmes conformes à cette version du standard. Les données sont échangées au format JSON par défaut, bien que le serveur supporte également XML. Le serveur est configuré avec CORS activé pour permettre les requêtes cross-origin depuis le frontend et les autres services, facilitant ainsi l'intégration dans une architecture distribuée.

\subsection{Configuration}

Le service HAPI FHIR est accessible sur le \textbf{port 8090}. Il expose l'endpoint principal \texttt{/fhir} pour toutes les opérations FHIR standard (CRUD sur les ressources).

\subsection{Diagramme BPMN}

Le diagramme BPMN du service HAPI FHIR (figure \ref{fig:bpmn-hapi-fhir}) illustre le processus de gestion des ressources FHIR.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{images/bpmn/hapi-fhir-bpmn.png}
\caption{Diagramme BPMN - HAPI FHIR Server}
\label{fig:bpmn-hapi-fhir}
\end{figure}

\subsection{Endpoints Principaux}

\begin{lstlisting}[language=bash, caption=Endpoints HAPI FHIR]
GET  /fhir/metadata              # CapabilityStatement
POST /fhir/Patient               # Créer un patient
GET  /fhir/Patient/{id}          # Lire un patient
PUT  /fhir/Patient/{id}          # Mettre à jour un patient
GET  /fhir/Encounter             # Liste des hospitalisations
GET  /fhir/Observation           # Liste des observations
GET  /fhir/Condition             # Liste des diagnostics
\end{lstlisting}

\newpage

\section{ProxyFHIR}

\subsection{Présentation}

Cette section présente ProxyFHIR, un service crucial qui fait le pont entre HAPI FHIR et le reste de l'écosystème HealthFlow-MS.

ProxyFHIR est un service Java/Spring Boot qui agit comme un proxy intelligent entre les autres services et HAPI FHIR. Son rôle principal consiste à synchroniser périodiquement les données FHIR depuis le serveur HAPI FHIR vers la base de données PostgreSQL locale, créant ainsi une copie locale des données pour améliorer les performances et réduire la dépendance au serveur FHIR externe. En plus de cette synchronisation, ProxyFHIR fournit une API REST pour accéder aux données synchronisées et agit également comme un proxy pour les requêtes de création de ressources FHIR, contournant ainsi les problèmes de CORS et simplifiant l'intégration avec le frontend.

\subsection{Technologies}

Le service ProxyFHIR est construit sur une stack technologique moderne et robuste qui combine la puissance de Java avec l'écosystème Spring. Le choix de \textbf{Java 17} comme langage de programmation n'est pas anodin : cette version LTS (Long Term Support) offre des performances exceptionnelles, une stabilité éprouvée, et des fonctionnalités modernes comme les records, les pattern matching, et les text blocks qui améliorent la productivité du développement tout en maintenant la compatibilité avec l'écosystème Java existant. Java 17 représente un équilibre optimal entre innovation et stabilité, essentiel pour un système critique dans le domaine médical.

\textbf{Spring Boot 3.2.0} constitue le framework principal du service, offrant une infrastructure complète pour le développement d'applications Java d'entreprise. Spring Boot simplifie considérablement la configuration et le déploiement en fournissant une configuration par défaut intelligente, une intégration native avec les bases de données, et un système de gestion des dépendances via Spring Dependency Injection. La version 3.2.0 apporte des améliorations significatives en termes de performance, notamment avec le support natif de GraalVM pour la compilation ahead-of-time, et des améliorations dans la gestion de la mémoire qui sont cruciales pour un service qui doit gérer de grandes quantités de données FHIR.

Le \textbf{Client FHIR HAPI 6.10.0} est l'outil spécialisé qui permet à ProxyFHIR de communiquer efficacement avec le serveur HAPI FHIR. Ce client est développé par la même équipe que le serveur HAPI FHIR, garantissant une compatibilité parfaite et une compréhension approfondie du standard FHIR. Le client gère automatiquement la sérialisation et la désérialisation des ressources FHIR, la gestion des versions, et les opérations de recherche complexes selon le standard FHIR Search. Cette abstraction permet au service ProxyFHIR de se concentrer sur la logique métier plutôt que sur les détails d'implémentation du protocole FHIR.

\textbf{Spring Data JPA} fournit une couche d'abstraction puissante pour l'accès aux données, simplifiant considérablement les opérations de persistance. Au lieu d'écrire manuellement des requêtes SQL complexes, Spring Data JPA permet de définir des interfaces de repository avec des méthodes qui sont automatiquement implémentées par le framework. Cette approche déclarative réduit le code boilerplate de manière significative tout en offrant la flexibilité nécessaire pour des requêtes personnalisées lorsque c'est requis. L'intégration avec Hibernate comme implémentation JPA apporte des fonctionnalités avancées comme le lazy loading, le caching de second niveau, et la gestion automatique des transactions.

Enfin, \textbf{PostgreSQL} sert de base de données relationnelle pour stocker toutes les données FHIR synchronisées. Le choix de PostgreSQL plutôt que d'autres bases de données relationnelles est motivé par sa robustesse, ses performances exceptionnelles pour les requêtes complexes, son support natif des types JSON qui facilite le stockage des ressources FHIR, et sa conformité ACID qui garantit l'intégrité des données médicales critiques. PostgreSQL offre également des fonctionnalités avancées comme le full-text search, les index GIN pour les données JSON, et le support des transactions distribuées, toutes des fonctionnalités qui sont exploitées par ProxyFHIR pour optimiser les performances et garantir la fiabilité.

\subsection{Diagramme BPMN}

Le diagramme BPMN du service ProxyFHIR (figure \ref{fig:bpmn-proxy-fhir}) montre le processus de synchronisation FHIR.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{images/bpmn/proxy-fhir-bpmn.png}
\caption{Diagramme BPMN - ProxyFHIR}
\label{fig:bpmn-proxy-fhir}
\end{figure}

\subsection{Fonctionnalités Principales}

ProxyFHIR implémente quatre fonctionnalités principales qui en font un composant essentiel de l'architecture. La synchronisation FHIR récupère automatiquement toutes les ressources depuis HAPI FHIR (patients, encounters, observations, conditions) et les stocke dans PostgreSQL pour un accès rapide et local. Le stockage persistant dans PostgreSQL permet de maintenir une copie locale des données FHIR, réduisant la latence et améliorant la disponibilité du système même en cas de problème avec HAPI FHIR. La fonctionnalité de proxy API permet de proxier les requêtes de création de ressources vers HAPI FHIR, contournant ainsi les restrictions CORS et simplifiant l'intégration avec le frontend. Enfin, un endpoint de healthcheck vérifie régulièrement l'état de santé du service et sa capacité à communiquer avec HAPI FHIR et PostgreSQL.

\subsection{Endpoints Principaux}

\begin{lstlisting}[language=bash, caption=Endpoints ProxyFHIR]
POST /api/fhir/sync              # Déclencher synchronisation
GET  /api/fhir/patients           # Liste des patients
GET  /api/fhir/patients/{id}      # Détails patient
GET  /api/fhir/encounters         # Liste des hospitalisations
GET  /api/fhir/stats              # Statistiques
POST /api/fhir/proxy/Patient      # Créer patient (proxy)
POST /api/fhir/proxy/Encounter    # Créer encounter (proxy)
POST /api/fhir/proxy/Observation  # Créer observation (proxy)
POST /api/fhir/proxy/Condition   # Créer condition (proxy)
GET  /health                      # Healthcheck
\end{lstlisting}

\newpage

\section{DeID - Service d'Anonymisation}

\subsection{Présentation}

Cette section décrit le service DeID, responsable de l'anonymisation des données selon les normes de confidentialité les plus strictes.

Le service DeID (De-Identification) anonymise les données des patients selon la méthode HIPAA Safe Harbor, une approche standardisée qui supprime ou modifie 18 identifiants spécifiés par la réglementation HIPAA (Health Insurance Portability and Accountability Act). Cette anonymisation est une étape critique dans le pipeline de traitement des données, car elle permet de transformer des données identifiables en données pseudonymisées qui peuvent être utilisées pour l'analyse et la recherche sans compromettre la confidentialité des patients. Le service génère un pseudo-ID unique pour chaque patient anonymisé, permettant de maintenir la traçabilité des données tout en préservant l'anonymat.

\subsection{Technologies}

Le service DeID est développé en \textbf{Python 3.11}, un choix stratégique motivé par la richesse de l'écosystème Python pour le traitement de données et la facilité d'intégration avec des bibliothèques spécialisées dans l'anonymisation et la protection des données. Python 3.11 apporte des améliorations significatives en termes de performance par rapport aux versions précédentes, avec des optimisations du compilateur bytecode et une meilleure gestion de la mémoire, ce qui est crucial pour un service qui doit traiter de grandes quantités de données médicales de manière efficace.

\textbf{FastAPI 0.109.0} a été sélectionné comme framework web pour sa combinaison unique de performance, de simplicité d'utilisation, et de fonctionnalités modernes. FastAPI est construit sur les épaules de Starlette et Pydantic, offrant des performances comparables à Node.js et Go tout en conservant la simplicité et la productivité de Python. L'un des avantages majeurs de FastAPI est sa génération automatique de documentation interactive via Swagger/OpenAPI, permettant aux développeurs et aux utilisateurs du système de comprendre et tester facilement les endpoints du service. La validation automatique des données via Pydantic garantit que toutes les données entrantes respectent les schémas définis, réduisant ainsi les risques d'erreurs et améliorant la sécurité.

\textbf{SQLAlchemy 2.0} constitue l'ORM (Object-Relational Mapping) utilisé pour interagir avec PostgreSQL. SQLAlchemy 2.0 représente une refonte majeure de la bibliothèque, introduisant un nouveau style d'API plus moderne et plus performant. Cette version offre une meilleure intégration avec les types hints de Python, une syntaxe plus intuitive pour les requêtes, et des performances améliorées grâce à une optimisation plus agressive des requêtes générées. SQLAlchemy gère automatiquement la connexion pooling, la gestion des transactions, et la synchronisation entre les objets Python et les tables de base de données, simplifiant ainsi considérablement le développement.

Le respect du \textbf{standard HIPAA Safe Harbor} n'est pas simplement une exigence technique mais un engagement éthique fondamental. Ce standard définit 18 identifiants directs qui doivent être supprimés ou transformés pour qu'un ensemble de données soit considéré comme anonymisé selon la réglementation HIPAA. Le service DeID implémente méticuleusement toutes les exigences de ce standard, allant de la suppression complète des noms et adresses jusqu'à la transformation des dates en années uniquement, en passant par la pseudonymisation des identifiants uniques. Cette implémentation rigoureuse garantit que les données anonymisées ne peuvent pas être utilisées pour ré-identifier les patients, même en combinant plusieurs sources d'information, respectant ainsi à la fois la lettre et l'esprit de la réglementation sur la protection des données de santé.

\subsection{Diagramme BPMN}

Le diagramme BPMN du service DeID (figure \ref{fig:bpmn-deid}) illustre le processus d'anonymisation.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{images/bpmn/deid-bpmn.png}
\caption{Diagramme BPMN - Service DeID}
\label{fig:bpmn-deid}
\end{figure}

\subsection{Identifiants Supprimés/Modifiés}

Le service DeID applique rigoureusement la méthode HIPAA Safe Harbor en supprimant ou modifiant systématiquement les 18 identifiants personnels protégés par la réglementation. Ces identifiants incluent les noms complets des patients, toutes les adresses géographiques détaillées (seul l'État est conservé pour certaines analyses), toutes les dates précises (seule l'année est conservée pour préserver l'utilité des données tout en protégeant la confidentialité), ainsi que tous les numéros de contact (téléphone, fax, email). Les identifiants administratifs et financiers sont également supprimés, incluant les numéros de sécurité sociale, les numéros de dossier médical, les numéros de compte bancaire, les numéros de licence ou certificat, et les numéros de carte de crédit. Les identifiants techniques et biométriques sont également traités, supprimant les identifiants de véhicule, les identifiants de dispositif médical, les URLs web, les adresses IP, les identifiants biométriques, et les photos de visage. Enfin, tout autre numéro ou identifiant unique susceptible d'identifier un patient est également supprimé ou modifié.

\subsection{Endpoints Principaux}

\begin{lstlisting}[language=bash, caption=Endpoints DeID]
POST /api/deid/patient           # Anonymiser un patient
GET  /api/deid/patient/{id}       # Obtenir patient anonymisé
GET  /health                     # Healthcheck
\end{lstlisting}

\newpage

\section{Featurizer - Extraction de Caractéristiques}

\subsection{Présentation}

Cette section présente le service Featurizer, qui transforme les données médicales brutes en caractéristiques exploitables par les algorithmes de Machine Learning.

Le service Featurizer est le cœur de la préparation des données pour le Machine Learning dans HealthFlow-MS. Il extrait plus de 30 caractéristiques (features) pertinentes des données médicales anonymisées, transformant ainsi des données structurées et non structurées en un vecteur de features numériques que le modèle XGBoost peut traiter. Le service combine intelligemment des données démographiques (âge, genre), cliniques (durée de séjour, historique d'hospitalisations), de laboratoire (résultats de tests), et d'analyse NLP (extraction d'informations depuis les notes cliniques). Cette extraction multi-sources permet de capturer la complexité de l'état de santé d'un patient et d'améliorer significativement la précision des prédictions.

\subsection{Technologies}

Le service Featurizer exploite la puissance de \textbf{Python 3.11} pour orchestrer un pipeline complexe d'extraction de caractéristiques qui combine des données structurées et non structurées. Python est particulièrement adapté à ce type de traitement de données grâce à son écosystème riche en bibliothèques scientifiques et de Machine Learning. La version 3.11 apporte des améliorations de performance significatives qui sont cruciales pour un service qui doit traiter de grandes quantités de données médicales et effectuer des calculs complexes d'extraction de features.

\textbf{FastAPI 0.109.0} fournit l'infrastructure web pour exposer les fonctionnalités d'extraction de features via une API REST moderne et performante. FastAPI est particulièrement adapté aux services de Machine Learning car il supporte nativement la validation de données complexes via Pydantic, permettant ainsi de définir des schémas stricts pour les requêtes et réponses. L'asynchronicité native de FastAPI permet également de gérer efficacement les opérations d'extraction de features qui peuvent être longues, en permettant au service de traiter plusieurs requêtes simultanément sans bloquer.

L'intégration de \textbf{BioBERT} via la bibliothèque \textbf{transformers} représente l'une des innovations majeures du service. BioBERT est un modèle de langage pré-entraîné spécialement conçu pour le domaine biomédical, ayant été entraîné sur des millions de documents médicaux et scientifiques. Contrairement aux modèles de langage génériques comme BERT standard, BioBERT comprend le vocabulaire médical spécialisé, les relations conceptuelles entre termes médicaux, et les nuances sémantiques propres au domaine de la santé. Cette spécialisation permet au service d'extraire des informations beaucoup plus précises et pertinentes des notes cliniques, transformant ainsi du texte libre en caractéristiques quantitatives exploitables par les modèles de prédiction.

\textbf{spaCy 3.7.2} complète BioBERT en fournissant des capacités avancées de traitement du langage naturel, notamment la reconnaissance d'entités nommées (NER) et l'analyse syntaxique. spaCy est particulièrement performant pour identifier et extraire des entités médicales spécifiques comme les noms de médicaments, les symptômes, les diagnostics, et les procédures médicales. Le modèle médical de spaCy a été entraîné sur des corpus médicaux spécialisés, lui permettant de reconnaître des entités qui seraient manquées par des modèles génériques. La combinaison de BioBERT pour la compréhension sémantique globale et de spaCy pour l'extraction précise d'entités crée un pipeline NLP puissant et complet.

Enfin, \textbf{PostgreSQL} stocke toutes les features extraites, permettant ainsi leur réutilisation ultérieure sans avoir à recalculer les features à chaque fois. Cette persistance est cruciale pour la performance du système, car l'extraction de features, particulièrement avec l'analyse NLP, peut être coûteuse en termes de temps de calcul. En stockant les features dans PostgreSQL, le système peut répondre rapidement aux demandes de prédiction en récupérant simplement les features pré-calculées plutôt que de les recalculer à chaque fois.

\subsection{Diagramme BPMN}

Le diagramme BPMN du service Featurizer (figure \ref{fig:bpmn-featurizer}) montre le processus d'extraction de features.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{images/bpmn/featurizer-bpmn.png}
\caption{Diagramme BPMN - Service Featurizer}
\label{fig:bpmn-featurizer}
\end{figure}

\subsection{Types de Features Extraites}

Le service Featurizer extrait des caractéristiques organisées en six catégories principales, chacune capturant un aspect différent de l'état de santé et de l'historique médical du patient.

Les features démographiques incluent l'âge à l'admission (calculé à partir de la date de naissance), le genre encodé numériquement pour l'entrée dans le modèle, et le groupe d'âge (catégorisé en tranches pour certaines analyses). Ces caractéristiques démographiques sont des prédicteurs importants du risque de réadmission, car certains groupes démographiques présentent des risques différents.

Les features cliniques capturent l'historique et le contexte de l'hospitalisation actuelle. La durée de séjour est un indicateur important de la complexité du cas. Le nombre d'hospitalisations précédentes sur différentes périodes (30 jours, 90 jours, 365 jours) reflète la fréquence des soins et peut indiquer des problèmes de santé chroniques. Le type d'hospitalisation (inpatient versus outpatient) influence également le risque de réadmission.

Les features de signes vitaux représentent l'état physiologique du patient au moment de l'admission. La fréquence cardiaque, la tension artérielle (systolique et diastolique), la fréquence respiratoire, la température corporelle, et la saturation en oxygène sont des indicateurs critiques de l'état de santé immédiat du patient.

Les features de laboratoire incluent les résultats de tests biologiques essentiels tels que l'hémoglobine (indicateur d'anémie), la créatinine (fonction rénale), les électrolytes (sodium, potassium), le glucose (diabète), et le nombre de globules blancs (WBC, indicateur d'infection). Ces valeurs sont souvent normalisées et agrégées (moyenne, dernière valeur, tendance).

Les features de comorbidités utilisent des indices validés cliniquement pour quantifier la charge de morbidité. L'indice de comorbidité de Charlson et le score d'Elixhauser sont des mesures standardisées qui pondèrent différentes conditions médicales selon leur impact sur la mortalité et la morbidité. La présence de conditions spécifiques comme le diabète, l'hypertension, ou l'insuffisance cardiaque est également encodée.

Les features NLP (Traitement du Langage Naturel) sont extraites des notes cliniques non structurées en utilisant BioBERT et spaCy. Le score de sentiment analyse le ton général des notes, le score d'urgence identifie le niveau d'urgence perçu par les cliniciens, et le score de complexité quantifie la complexité du cas. Le nombre d'entités extraites (médicaments, symptômes, maladies) et les mentions spécifiques de médicaments et symptômes enrichissent significativement les données disponibles pour la prédiction.

\subsection{Endpoints Principaux}

\begin{lstlisting}[language=bash, caption=Endpoints Featurizer]
POST /api/features/extract        # Extraire features
GET  /api/features/{patient_id}   # Obtenir features
POST /api/nlp/analyze             # Analyser texte clinique
POST /api/nlp/entities            # Extraire entités
GET  /health                      # Healthcheck
\end{lstlisting}

\newpage

\section{ModelRisque - Prédiction ML}

\subsection{Présentation}

Le service ModelRisque utilise un modèle XGBoost pour prédire le risque de réadmission à 30 jours. Il fournit également des explications SHAP pour comprendre les facteurs contribuant à la prédiction.

\subsection{Technologies}

Le service ModelRisque est construit sur une stack technologique spécialisée pour le Machine Learning et l'explicabilité, exploitant la puissance de \textbf{Python 3.11} comme langage de programmation. Python est le choix naturel pour les services ML grâce à son écosystème exceptionnellement riche en bibliothèques de Machine Learning, de traitement de données, et d'analyse statistique. La version 3.11 apporte des améliorations significatives de performance qui sont cruciales pour un service qui doit effectuer des prédictions en temps réel, avec des optimisations du compilateur bytecode qui réduisent le temps d'exécution des opérations mathématiques intensives nécessaires pour les calculs de prédiction.

\textbf{FastAPI 0.109.0} fournit l'infrastructure web moderne et performante pour exposer les fonctionnalités de prédiction via une API REST. FastAPI est particulièrement adapté aux services ML car il supporte nativement la validation de données complexes via Pydantic, permettant de définir des schémas stricts pour les requêtes de prédiction qui garantissent que toutes les features nécessaires sont présentes et dans le bon format. L'asynchronicité native de FastAPI permet également de gérer efficacement les opérations de prédiction qui peuvent prendre plusieurs secondes, en permettant au service de traiter plusieurs requêtes simultanément sans bloquer.

\textbf{XGBoost 2.0.3} constitue le cœur algorithmique du service, fournissant le modèle de Machine Learning utilisé pour les prédictions de risque. XGBoost est un algorithme d'ensemble basé sur des arbres de décision qui combine les prédictions de plusieurs arbres faibles pour créer un modèle fort et performant. La version 2.0.3 apporte des améliorations significatives en termes de performance et de fonctionnalités, notamment un meilleur support pour les features catégorielles, des optimisations de la gestion de la mémoire, et des améliorations dans la gestion des valeurs manquantes. XGBoost a été choisi pour sa capacité à gérer efficacement des datasets avec de nombreuses features, sa robustesse face au surapprentissage grâce à la régularisation intégrée, et sa capacité à capturer des interactions complexes entre les features.

\textbf{SHAP 0.44.1} (SHapley Additive exPlanations) fournit les capacités d'explicabilité qui sont essentielles pour rendre les prédictions du modèle compréhensibles et interprétables par les cliniciens. SHAP calcule des valeurs qui expliquent la contribution de chaque feature à la prédiction finale, basées sur la théorie des jeux coopératifs. Cette explicabilité n'est pas un luxe mais une nécessité absolue dans le domaine médical, où les décisions basées sur des prédictions opaques ne peuvent pas être acceptées. SHAP permet aux cliniciens de comprendre non seulement quel est le niveau de risque d'un patient, mais également pourquoi ce niveau de risque a été calculé, quels sont les facteurs qui contribuent le plus au risque, et comment ces facteurs interagissent pour produire la prédiction finale.

Enfin, \textbf{PostgreSQL} stocke toutes les prédictions générées, permettant ainsi leur récupération ultérieure sans avoir à recalculer les prédictions à chaque fois. Cette persistance est cruciale pour la performance et l'auditabilité du système, car elle permet de maintenir un historique complet de toutes les prédictions effectuées, de comparer les prédictions au fil du temps, et de valider le modèle en comparant les prédictions avec les résultats réels observés. PostgreSQL offre également des fonctionnalités avancées comme le full-text search et les index GIN qui sont exploitées pour optimiser les requêtes sur les prédictions stockées.

\subsection{Diagramme BPMN}

Le diagramme BPMN du service ModelRisque (figure \ref{fig:bpmn-model-risque}) illustre le processus de prédiction.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{images/bpmn/model-risque-bpmn.png}
\caption{Diagramme BPMN - Service ModelRisque}
\label{fig:bpmn-model-risque}
\end{figure}

\subsection{Modèle XGBoost}

\subsubsection{Hyperparamètres}

\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Paramètre} & \textbf{Valeur} \\
\hline
n\_estimators & 500 \\
max\_depth & 6 \\
learning\_rate & 0.05 \\
subsample & 0.8 \\
colsample\_bytree & 0.8 \\
min\_child\_weight & 1 \\
reg\_alpha & 0.01 \\
reg\_lambda & 1 \\
\hline
\end{tabular}
\caption{Hyperparamètres du modèle XGBoost}
\end{table}

\subsubsection{Métriques de Performance}

\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Métrique} & \textbf{Valeur} \\
\hline
AUC-ROC & 0.82 \\
Précision & 0.78 \\
Rappel & 0.74 \\
F1-Score & 0.76 \\
\hline
\end{tabular}
\caption{Métriques de performance du modèle}
\end{table}

\subsection{Explicabilité SHAP}

L'explicabilité est un aspect crucial des modèles de Machine Learning dans le domaine médical, où la transparence des décisions est essentielle pour la confiance des cliniciens et la conformité réglementaire.

SHAP (SHapley Additive exPlanations) est une méthode d'explication basée sur la théorie des jeux coopératifs qui fournit des valeurs précises expliquant la contribution de chaque feature au score de risque final. Les valeurs SHAP sont calculées en évaluant l'impact de chaque feature dans toutes les combinaisons possibles avec les autres features, garantissant ainsi une attribution équitable de la contribution. Les valeurs SHAP positives indiquent que la feature augmente le risque de réadmission, tandis que les valeurs négatives indiquent qu'elle diminue le risque. La magnitude de la valeur SHAP reflète l'importance relative de la contribution, permettant aux cliniciens d'identifier rapidement les facteurs de risque les plus significatifs pour chaque patient.

\subsection{Endpoints Principaux}

\begin{lstlisting}[language=bash, caption=Endpoints ModelRisque]
POST /api/predict                 # Prédire le risque
GET  /api/predict/{patient_id}    # Obtenir prédiction
GET  /api/shap/{patient_id}       # Explication SHAP
GET  /health                      # Healthcheck
\end{lstlisting}

\newpage

\section{ScoreAPI - API Principale}

\subsection{Présentation}

ScoreAPI est l'API REST principale du système. Elle fournit un point d'accès unifié et sécurisé pour tous les services, gère l'authentification JWT, et expose les scores de risque et les explications.

\subsection{Technologies}

ScoreAPI exploite la puissance de \textbf{Python 3.11} pour orchestrer l'accès unifié à tous les services du système. Python est particulièrement adapté pour ce rôle d'orchestration car il offre une excellente intégration avec les bibliothèques HTTP pour communiquer avec les autres microservices, des capacités avancées de traitement de données pour agréger et transformer les réponses des différents services, et une syntaxe claire qui facilite la maintenance et l'évolution du code d'orchestration complexe.

\textbf{FastAPI 0.109.0} constitue le framework web qui expose l'API REST principale du système. FastAPI est idéal pour ce rôle car il combine performance, simplicité d'utilisation, et fonctionnalités modernes comme la génération automatique de documentation interactive, la validation automatique des données, et le support natif de l'asynchronicité. Ces fonctionnalités permettent à ScoreAPI de gérer efficacement les requêtes complexes qui nécessitent des appels à plusieurs microservices, tout en maintenant une API claire et bien documentée pour les clients.

L'\textbf{authentification JWT} via \textbf{python-jose} garantit que seuls les utilisateurs autorisés peuvent accéder aux données sensibles du système. JWT (JSON Web Tokens) est un standard moderne d'authentification stateless qui évite de maintenir des sessions serveur, simplifiant ainsi la scalabilité horizontale du système. python-jose implémente le standard JWT de manière complète et sécurisée, supportant plusieurs algorithmes de signature (RS256, HS256) et permettant une validation robuste des tokens. Cette implémentation garantit que les tokens ne peuvent pas être falsifiés et que leur expiration est correctement gérée.

La sécurité des mots de passe est assurée par \textbf{bcrypt}, un algorithme de hachage spécialement conçu pour les mots de passe. Contrairement aux algorithmes de hachage rapides comme MD5 ou SHA-256, bcrypt est intentionnellement lent et coûteux en termes de calcul, rendant ainsi les attaques par force brute pratiquement impossibles même avec des mots de passe faibles. bcrypt adapte automatiquement le coût du hachage en fonction de la puissance de calcul disponible, garantissant que la sécurité reste forte même avec l'évolution de la technologie. Cette approche garantit que même si la base de données est compromise, les mots de passe ne peuvent pas être récupérés en clair.

\textbf{PostgreSQL} stocke toutes les données nécessaires pour le fonctionnement de ScoreAPI, incluant les utilisateurs, les tokens de rafraîchissement, les logs d'audit, et les métadonnées des prédictions. PostgreSQL offre la robustesse et les performances nécessaires pour gérer les opérations critiques d'authentification et d'autorisation, avec des garanties ACID qui assurent la cohérence des données même en cas de pannes. Les fonctionnalités avancées de PostgreSQL comme les transactions, les contraintes d'intégrité, et les index sont exploitées pour garantir la sécurité et la performance du système d'authentification.

\subsection{Diagramme BPMN}

Le diagramme BPMN du service ScoreAPI (figure \ref{fig:bpmn-score-api}) montre le processus d'authentification et d'accès aux données.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{images/bpmn/score-api-bpmn.png}
\caption{Diagramme BPMN - Service ScoreAPI}
\label{fig:bpmn-score-api}
\end{figure}

\subsection{Fonctionnalités Principales}

ScoreAPI offre un ensemble complet de fonctionnalités qui font de lui le point d'entrée central et sécurisé pour tous les clients du système. L'\textbf{authentification} via login/logout avec JWT constitue la première ligne de défense du système, garantissant que seuls les utilisateurs autorisés peuvent accéder aux données sensibles. Le processus d'authentification est conçu pour être à la fois sécurisé et convivial : les utilisateurs s'authentifient une fois avec leurs identifiants, reçoivent un token JWT qui est valide pendant une période déterminée, et peuvent utiliser ce token pour accéder à toutes les ressources autorisées sans avoir à se ré-authentifier constamment. Le système de refresh tokens permet de renouveler les tokens d'accès sans demander à l'utilisateur de saisir à nouveau ses identifiants, améliorant ainsi l'expérience utilisateur tout en maintenant la sécurité.

La \textbf{gestion des utilisateurs} via des opérations CRUD complètes permet aux administrateurs de gérer efficacement les comptes utilisateurs du système. Cette fonctionnalité inclut la création de nouveaux utilisateurs avec des rôles et permissions appropriés, la mise à jour des informations utilisateur, la désactivation de comptes qui ne sont plus nécessaires, et la consultation de la liste de tous les utilisateurs. Chaque opération est soumise à des contrôles d'autorisation stricts qui garantissent que seuls les utilisateurs avec les permissions appropriées peuvent effectuer des modifications, et toutes les opérations sont enregistrées dans les logs d'audit pour garantir la traçabilité.

L'accès aux \textbf{scores de risque} constitue la fonctionnalité principale de ScoreAPI, permettant aux clients de récupérer les prédictions de risque pour les patients. Cette fonctionnalité ne se contente pas de retourner un simple score numérique, mais fournit une information complète incluant le score de risque (une probabilité entre 0 et 1), le niveau de risque catégorisé (FAIBLE, MOYEN, ÉLEVÉ), l'intervalle de confiance qui quantifie l'incertitude de la prédiction, et la date de la prédiction. Cette information complète permet aux cliniciens de prendre des décisions éclairées basées non seulement sur le niveau de risque, mais également sur la confiance du modèle dans sa prédiction.

L'accès aux \textbf{explications SHAP} complète les scores de risque en fournissant une compréhension détaillée des facteurs qui contribuent à chaque prédiction. Ces explications identifient les features qui augmentent le risque (valeurs SHAP positives) et celles qui le diminuent (valeurs SHAP négatives), avec une quantification de l'importance relative de chaque contribution. Cette transparence est essentielle pour la confiance clinique et permet aux médecins de comprendre non seulement quel est le risque, mais également pourquoi le modèle a calculé ce niveau de risque spécifique, facilitant ainsi l'interprétation clinique et la prise de décision.

Le \textbf{dashboard} avec statistiques agrégées offre une vue d'ensemble du système qui permet aux administrateurs et aux gestionnaires de surveiller l'état global du système. Ces statistiques incluent le nombre total de patients, le nombre de patients à risque élevé nécessitant une attention immédiate, le nombre de prédictions effectuées, la précision moyenne du modèle, et la distribution des risques parmi tous les patients. Ces métriques agrégées permettent une compréhension rapide de la situation globale et facilitent la prise de décisions stratégiques basées sur des données.

Enfin, le système d'\textbf{audit} avec logs de toutes les actions garantit la traçabilité complète de toutes les opérations effectuées dans le système. Chaque action, qu'il s'agisse d'une authentification, d'une consultation de données, d'une modification, ou d'une prédiction, est enregistrée dans les logs d'audit avec des informations détaillées incluant l'utilisateur qui a effectué l'action, l'heure exacte, les données concernées, et le résultat de l'opération. Ces logs sont essentiels pour la conformité réglementaire, la détection d'anomalies, la résolution de problèmes, et la reconstruction de l'historique des actions en cas de besoin.

\subsection{Endpoints Principaux}

\begin{lstlisting}[language=bash, caption=Endpoints ScoreAPI]
# Authentification
POST /api/v1/auth/login           # Connexion
POST /api/v1/auth/refresh        # Rafraîchir token
GET  /api/v1/auth/me             # Utilisateur actuel

# Patients
GET  /api/v1/patients            # Liste patients
GET  /api/v1/patients/{id}       # Détails patient
GET  /api/v1/patients/{id}/risk-score  # Score de risque
GET  /api/v1/patients/{id}/risk-explanation  # Explication
DELETE /api/v1/patients/{id}     # Supprimer patient

# Dashboard
GET  /api/v1/dashboard/stats     # Statistiques

# Utilisateurs
POST /api/v1/users               # Créer utilisateur
GET  /api/v1/users              # Liste utilisateurs

# Audit
GET  /api/v1/audit/logs          # Logs d'audit
\end{lstlisting}

\newpage

\section{AuditFairness - Dashboard d'Audit}

\subsection{Présentation}

Le service AuditFairness fournit un dashboard interactif pour visualiser les métriques de fairness et détecter les biais dans les prédictions du modèle. Il utilise EvidentlyAI pour calculer les métriques de fairness.

\subsection{Technologies}

Le service AuditFairness exploite \textbf{Python 3.11} pour orchestrer un dashboard interactif sophistiqué qui combine l'analyse de données, la visualisation, et l'interactivité web. Python est le choix naturel pour ce type d'application car il offre une intégration exceptionnelle avec les bibliothèques de data science et de visualisation, permettant ainsi de créer des dashboards riches et interactifs sans avoir à écrire du code JavaScript complexe.

\textbf{Dash 2.14.2} construit sur \textbf{Flask 3.0} constitue le framework principal pour créer le dashboard interactif. Dash est une bibliothèque Python révolutionnaire qui permet de créer des applications web interactives entièrement en Python, sans avoir à écrire de code HTML, CSS, ou JavaScript. Dash utilise React sous le capot pour gérer l'interface utilisateur, mais expose une API Python simple et intuitive qui permet aux développeurs de créer des dashboards sophistiqués en définissant simplement la structure et le comportement des composants. Cette approche permet de créer rapidement des dashboards personnalisés qui s'adaptent aux besoins spécifiques du système, tout en bénéficiant de la puissance et de la flexibilité de React pour l'interactivité et les mises à jour en temps réel.

\textbf{EvidentlyAI 0.4.33} est la bibliothèque spécialisée qui calcule les métriques de fairness et détecte les biais dans les prédictions du modèle. EvidentlyAI est une bibliothèque open-source conçue spécifiquement pour l'analyse de la qualité des modèles ML et la détection de biais, offrant des métriques standardisées et des visualisations claires qui facilitent l'interprétation des résultats. La bibliothèque calcule automatiquement plusieurs métriques de fairness (Demographic Parity, Equalized Odds, Equal Opportunity, Calibration) et génère des rapports détaillés qui identifient les biais potentiels et quantifient leur magnitude. Cette automatisation est cruciale car elle permet de surveiller continuellement la fairness du modèle sans nécessiter une expertise approfondie en statistiques de la part des utilisateurs.

\textbf{Plotly} fournit les capacités de visualisation avancées qui rendent les métriques de fairness compréhensibles et actionnables. Plotly est une bibliothèque de visualisation puissante qui supporte une large gamme de types de graphiques, depuis les graphiques de base comme les barres et les lignes jusqu'aux visualisations complexes comme les heatmaps, les graphiques 3D, et les graphiques interactifs. L'intégration native de Plotly avec Dash permet de créer des visualisations interactives où les utilisateurs peuvent zoomer, filtrer, et explorer les données de manière intuitive. Cette interactivité est essentielle pour comprendre les patterns complexes dans les métriques de fairness et identifier les sources de biais.

Enfin, \textbf{PostgreSQL} stocke toutes les données nécessaires pour le calcul des métriques de fairness, incluant les prédictions, les résultats réels observés, et les métadonnées démographiques des patients. PostgreSQL offre les performances et la robustesse nécessaires pour gérer les requêtes complexes qui agrègent les données sur différents groupes démographiques et calculent les métriques de fairness. Les fonctionnalités avancées de PostgreSQL comme les vues matérialisées et les index partiels sont exploitées pour optimiser les performances des calculs de fairness qui peuvent être coûteux en termes de temps de calcul.

\subsection{Diagramme BPMN}

Le diagramme BPMN du service AuditFairness (figure \ref{fig:bpmn-audit-fairness}) illustre le processus d'audit et de détection de biais.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{images/bpmn/audit-fairness-bpmn.png}
\caption{Diagramme BPMN - Service AuditFairness}
\label{fig:bpmn-audit-fairness}
\end{figure}

\subsection{Métriques de Fairness}

Le service calcule plusieurs métriques de fairness sophistiquées qui permettent de détecter et quantifier les biais potentiels dans les prédictions du modèle. La métrique de \textbf{Demographic Parity} mesure le ratio des taux de prédiction positive entre différents groupes démographiques (par exemple, hommes versus femmes, différents groupes d'âge). Cette métrique est cruciale car elle garantit que le modèle ne discrimine pas systématiquement certains groupes en leur attribuant des taux de risque différents sans justification médicale. Un ratio proche de 1 indique que le modèle prédit des risques similaires pour tous les groupes, tandis qu'un ratio significativement différent de 1 suggère un biais potentiel qui nécessite une investigation approfondie.

La métrique d'\textbf{Equalized Odds} va plus loin en vérifiant la parité des taux de vrais positifs et de faux positifs entre les groupes. Cette métrique est particulièrement importante car elle garantit que le modèle a la même précision pour tous les groupes démographiques, non seulement en termes de taux de prédiction positive globale, mais également en termes de capacité à identifier correctement les patients à haut risque (vrais positifs) et à éviter les faux alarmes (faux positifs). Un modèle qui a des taux de vrais positifs différents selon les groupes peut avoir des conséquences graves, car certains groupes pourraient être sous-diagnostiqués ou sur-diagnostiqués.

\textbf{Equal Opportunity} se concentre spécifiquement sur la parité des taux de vrais positifs, assurant que les patients à haut risque sont identifiés avec la même précision dans tous les groupes démographiques. Cette métrique est fondamentale car elle garantit l'équité dans l'accès aux interventions préventives : tous les patients à haut risque, indépendamment de leur groupe démographique, doivent avoir la même probabilité d'être correctement identifiés par le modèle. Un déséquilibre dans cette métrique pourrait indiquer que le modèle a appris des patterns qui reflètent des biais historiques dans les données d'entraînement plutôt que des différences médicales réelles.

Enfin, la \textbf{Calibration} vérifie la cohérence des probabilités prédites avec les résultats réels observés. Un modèle bien calibré est un modèle où, lorsque le modèle prédit un risque de 70\%, environ 70\% des patients sont effectivement réadmis. Cette cohérence est essentielle pour la confiance clinique : les médecins doivent pouvoir interpréter les scores de risque comme de véritables probabilités, pas seulement comme des scores relatifs. Un modèle mal calibré peut conduire à des décisions cliniques sous-optimales, car les cliniciens pourraient surestimer ou sous-estimer le risque réel basé sur des prédictions qui ne reflètent pas fidèlement la probabilité réelle de réadmission.

\subsection{Endpoints Principaux}

\begin{lstlisting}[language=bash, caption=Endpoints AuditFairness]
GET  /                              # Dashboard principal
GET  /fairness                      # Métriques de fairness
GET  /bias                          # Détection de biais
GET  /health                        # Healthcheck
\end{lstlisting}

\newpage

\section{Frontend - Interface Utilisateur}

\subsection{Présentation}

Le Frontend est une application web moderne développée avec React et TypeScript. Il fournit une interface intuitive pour interagir avec tous les services du système.

\subsection{Technologies}

Le Frontend est développé en \textbf{TypeScript}, un sur-ensemble typé de JavaScript qui apporte la sécurité de type et l'autocomplétion intelligente au développement web. TypeScript est particulièrement précieux pour un projet de cette envergure car il permet de détecter les erreurs à la compilation plutôt qu'à l'exécution, réduisant ainsi considérablement les bugs et améliorant la maintenabilité du code. Le système de types de TypeScript permet également de documenter implicitement les interfaces et les structures de données, facilitant ainsi la collaboration entre développeurs et réduisant le temps nécessaire pour comprendre le code existant.

\textbf{React 18.2} constitue le framework principal pour la construction de l'interface utilisateur. React est le choix naturel pour une application moderne car il offre une architecture basée sur les composants qui permet de construire des interfaces complexes de manière modulaire et réutilisable. La version 18.2 apporte des améliorations significatives en termes de performance, notamment avec le support natif des composants asynchrones, l'amélioration du système de rendu concurrent, et l'optimisation automatique des mises à jour de l'interface. React permet également de créer des interfaces réactives qui se mettent à jour automatiquement lorsque les données changent, offrant ainsi une expérience utilisateur fluide et moderne.

\textbf{Vite 5.0} est utilisé comme outil de build et de développement, remplaçant les outils traditionnels comme Webpack par une solution moderne et ultra-rapide. Vite révolutionne l'expérience de développement en utilisant les modules ES natifs du navigateur pour le développement, permettant ainsi des rechargements instantanés même pour de grandes applications. Pour la production, Vite utilise Rollup pour créer des bundles optimisés qui sont beaucoup plus petits et plus rapides que ceux générés par les outils traditionnels. Cette amélioration de performance est cruciale pour une application qui doit être accessible rapidement même sur des connexions lentes.

\textbf{Tailwind CSS} fournit le système de styling utilitaire qui permet de créer des interfaces modernes et cohérentes sans avoir à écrire de CSS personnalisé. Tailwind CSS adopte une approche "utility-first" où les styles sont appliqués directement dans le HTML via des classes prédéfinies, permettant ainsi de créer rapidement des interfaces complexes tout en maintenant une cohérence visuelle. Cette approche réduit considérablement le temps de développement et facilite la maintenance, car les styles sont déclarés directement là où ils sont utilisés plutôt que dans des fichiers CSS séparés.

\textbf{Recharts} est utilisé pour créer les graphiques et visualisations de données qui sont essentiels pour afficher les statistiques, les distributions de risques, et les tendances. Recharts est une bibliothèque de graphiques construite spécifiquement pour React, offrant une intégration native et une API déclarative qui s'intègre naturellement avec le paradigme de composants de React. La bibliothèque supporte une large gamme de types de graphiques (lignes, barres, camemberts, aires) et offre des fonctionnalités avancées comme les tooltips interactifs, les animations, et la responsivité automatique.

\textbf{Axios} est utilisé comme client HTTP pour communiquer avec les APIs backend. Axios est préféré à l'API fetch native car il offre des fonctionnalités avancées comme l'interception des requêtes et réponses (permettant d'ajouter automatiquement les tokens JWT), la gestion automatique des erreurs, la transformation automatique des données JSON, et le support des timeouts. Ces fonctionnalités simplifient considérablement le code de communication avec les APIs et améliorent la robustesse de l'application.

Enfin, \textbf{Nginx} est utilisé comme serveur web en production pour servir l'application React compilée. Nginx est un serveur web haute performance qui peut servir efficacement les fichiers statiques de l'application React, gérer le cache, et même agir comme reverse proxy pour les APIs backend. Cette configuration permet de déployer l'application de manière optimale avec des performances maximales et une utilisation minimale des ressources serveur.

\subsection{Diagramme BPMN}

Le diagramme BPMN du Frontend (figure \ref{fig:bpmn-frontend}) montre le processus d'interaction utilisateur.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{images/bpmn/frontend-bpmn.png}
\caption{Diagramme BPMN - Frontend}
\label{fig:bpmn-frontend}
\end{figure}

\subsection{Fonctionnalités Principales}

Le Frontend offre un ensemble complet de fonctionnalités qui rendent le système accessible et utilisable par tous les professionnels de santé, indépendamment de leur niveau technique. L'\textbf{authentification} avec login/logout et gestion de session constitue le point d'entrée sécurisé du système. Le processus d'authentification est conçu pour être intuitif et sécurisé : les utilisateurs saisissent leurs identifiants une fois, le système valide leurs credentials auprès de ScoreAPI, et reçoit un token JWT qui est stocké de manière sécurisée dans le navigateur. Ce token est automatiquement inclus dans toutes les requêtes suivantes, permettant ainsi un accès transparent aux ressources autorisées. La gestion de session inclut également la détection automatique de l'expiration du token et la redirection vers la page de connexion lorsque nécessaire, garantissant ainsi que les utilisateurs ne restent pas bloqués avec un token expiré.

Le \textbf{dashboard} offre une vue d'ensemble complète du système avec des statistiques agrégées qui permettent aux utilisateurs de comprendre rapidement l'état global du système. Le dashboard affiche des métriques clés comme le nombre total de patients, le nombre de patients à risque élevé nécessitant une attention immédiate, le nombre de prédictions effectuées aujourd'hui, et la précision moyenne du modèle. Ces métriques sont présentées sous forme de cartes visuelles qui sont faciles à comprendre au premier coup d'œil. Le dashboard inclut également des graphiques interactifs qui visualisent la distribution des risques parmi tous les patients, permettant ainsi de comprendre rapidement les tendances et les patterns dans les données. Cette vue d'ensemble est essentielle pour les administrateurs et les gestionnaires qui doivent prendre des décisions stratégiques basées sur des données.

La \textbf{gestion des patients} offre des fonctionnalités complètes pour gérer la liste des patients, créer de nouveaux patients, et supprimer des patients qui ne sont plus nécessaires. La liste des patients est présentée dans un tableau interactif qui permet de filtrer, trier, et rechercher parmi les patients selon différents critères. Les utilisateurs peuvent filtrer pour afficher uniquement les patients à risque élevé, ceux sans score encore calculé, ou ceux dans une certaine tranche d'âge. La création de nouveaux patients est facilitée par un formulaire guidé qui valide les données en temps réel et fournit des messages d'erreur clairs lorsque des données invalides sont saisies. La suppression de patients est protégée par des confirmations pour éviter les suppressions accidentelles, garantissant ainsi l'intégrité des données.

Le \textbf{workflow complet} offre un processus guidé étape par étape qui permet aux utilisateurs de créer un patient et d'obtenir son score de risque de manière intuitive et structurée. Ce workflow guide l'utilisateur à travers toutes les étapes nécessaires : création du patient, ajout des données médicales (hospitalisations, signes vitaux, diagnostics), anonymisation, extraction de features, et calcul du risque. Chaque étape est clairement expliquée avec des instructions et des exemples, et le système valide les données à chaque étape avant de permettre de passer à l'étape suivante. Ce processus guidé est particulièrement précieux pour les nouveaux utilisateurs qui ne sont pas encore familiers avec le système, car il les guide à travers toutes les étapes nécessaires sans les laisser se perdre.

La \textbf{visualisation} avec graphiques et tableaux de données rend les informations complexes accessibles et compréhensibles. Les graphiques utilisent Recharts pour créer des visualisations interactives où les utilisateurs peuvent zoomer, filtrer, et explorer les données de manière intuitive. Les tableaux de données sont présentés avec des fonctionnalités avancées comme le tri, la recherche, et la pagination, permettant ainsi de naviguer efficacement même dans de grandes listes de patients. Les explications SHAP sont présentées de manière visuelle avec des graphiques qui montrent clairement quelles features contribuent positivement ou négativement au risque, facilitant ainsi l'interprétation clinique des prédictions.

Enfin, le système de \textbf{notifications} avec toasts pour les retours utilisateur garantit que les utilisateurs sont toujours informés du résultat de leurs actions. Les toasts sont des notifications non-intrusives qui apparaissent temporairement dans un coin de l'écran pour informer l'utilisateur du succès ou de l'échec d'une opération, sans interrompre leur workflow. Ce système de feedback est essentiel pour une bonne expérience utilisateur, car il permet aux utilisateurs de savoir immédiatement si leurs actions ont réussi ou échoué, sans avoir à vérifier manuellement le résultat. Les notifications sont également utilisées pour informer les utilisateurs des erreurs avec des messages clairs qui expliquent ce qui s'est mal passé et comment corriger le problème.

\subsection{Pages Principales}

L'interface frontend comprend plusieurs pages principales organisées selon les besoins des différents rôles utilisateurs, chacune optimisée pour une tâche spécifique et offrant une expérience utilisateur fluide et intuitive. La page de connexion (\texttt{/login}) constitue le point d'entrée sécurisé du système, permettant l'authentification sécurisée avec gestion des tokens JWT. Cette page est conçue pour être simple et claire, avec un formulaire de connexion qui valide les identifiants en temps réel et fournit des messages d'erreur clairs en cas d'échec d'authentification. Une fois authentifié, l'utilisateur est automatiquement redirigé vers le dashboard principal, et son token JWT est stocké de manière sécurisée pour les sessions futures.

Le dashboard principal (\texttt{/}) offre une vue d'ensemble complète du système avec des statistiques agrégées, des graphiques de distribution des risques, et des indicateurs clés de performance. Cette page est conçue pour donner aux utilisateurs une compréhension immédiate de l'état global du système, avec des métriques clés présentées sous forme de cartes visuelles faciles à comprendre. Les graphiques interactifs permettent d'explorer les données de manière détaillée, avec des fonctionnalités de zoom et de filtrage qui permettent d'analyser les tendances et les patterns. Cette vue d'ensemble est essentielle pour les administrateurs et les gestionnaires qui doivent prendre des décisions stratégiques basées sur des données.

La page de liste des patients (\texttt{/patients}) permet de visualiser tous les patients avec leurs scores de risque, avec des filtres sophistiqués pour afficher uniquement les patients à risque élevé, ceux sans score encore calculé, ou ceux répondant à d'autres critères spécifiques. Le tableau de patients est interactif et permet de trier, rechercher, et filtrer selon de nombreux critères différents. Chaque ligne du tableau affiche les informations essentielles du patient, et un clic sur une ligne permet d'accéder rapidement aux détails complets. Cette page est optimisée pour la navigation efficace même avec de grandes listes de patients, avec pagination et chargement paresseux qui garantissent des performances fluides.

La page de détails d'un patient (\texttt{/patients/:id}) affiche toutes les informations détaillées d'un patient spécifique, incluant son historique médical complet, ses features extraites par le service Featurizer, et les explications SHAP détaillées de son score de risque. Cette page est organisée en sections logiques qui permettent de naviguer facilement entre les différentes catégories d'informations. Les explications SHAP sont présentées de manière visuelle avec des graphiques qui montrent clairement quelles features contribuent positivement ou négativement au risque, facilitant ainsi l'interprétation clinique. Cette page est essentielle pour les cliniciens qui doivent comprendre en détail l'état d'un patient et les facteurs qui influencent son risque de réadmission.

Le workflow complet guidé (\texttt{/patient-workflow}) offre un processus étape par étape pour créer un patient, ajouter ses données médicales, anonymiser, extraire les features, et obtenir une prédiction de risque. Ce workflow est conçu pour être intuitif même pour les utilisateurs non techniques, avec des instructions claires à chaque étape et une validation automatique des données qui empêche les erreurs avant qu'elles ne se produisent. Le processus est visuellement guidé avec un indicateur de progression qui montre clairement où l'utilisateur se trouve dans le workflow, et chaque étape est expliquée en détail avant d'être exécutée. Ce processus guidé est particulièrement précieux pour les nouveaux utilisateurs qui apprennent à utiliser le système.

Enfin, la page de visualisation des prédictions (\texttt{/predictions}) permet d'analyser les prédictions de manière agrégée avec des graphiques et des tableaux de données qui révèlent les patterns et les tendances dans les prédictions. Cette page est conçue pour les analyses approfondies, avec des outils de filtrage et d'agrégation avancés qui permettent d'explorer les données sous différents angles. Les visualisations interactives permettent de zoomer, filtrer, et explorer les données de manière intuitive, révélant ainsi des insights qui ne seraient pas visibles dans une simple liste. Cette page est essentielle pour les analystes et les chercheurs qui doivent comprendre les patterns dans les prédictions et identifier les opportunités d'amélioration du modèle.

\subsection{Captures d'Écran de l'Interface Admin}

Cette section présente les principales interfaces de l'application frontend développée par le Groupe 8 de l'EMSI, illustrant les fonctionnalités disponibles pour les administrateurs et les cliniciens. Ces captures d'écran démontrent l'interface utilisateur moderne et intuitive qui facilite l'utilisation du système HealthFlow-MS.

\subsubsection{Page de Connexion}

La figure \ref{fig:frontend-login} présente la page de connexion de l'application. Cette interface minimaliste permet aux utilisateurs de s'authentifier avec leur nom d'utilisateur et mot de passe. Le système utilise l'authentification JWT pour sécuriser l'accès, et les tokens sont stockés localement pour maintenir la session entre les rafraîchissements de page. La page affiche un formulaire simple et élégant avec validation en temps réel des champs, offrant une expérience utilisateur fluide et professionnelle.

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{images/frontend/login.png}
\caption{Page de connexion - Interface d'authentification sécurisée}
\label{fig:frontend-login}
\end{figure}

\subsubsection{Dashboard Administrateur}

La figure \ref{fig:frontend-dashboard} présente le dashboard principal accessible aux administrateurs et cliniciens. Cette page offre une vue d'ensemble complète du système avec plusieurs métriques clés affichées sous forme de cartes statistiques. Le dashboard inclut le nombre total de patients, le nombre de patients à risque élevé nécessitant une attention immédiate, le nombre de prédictions effectuées aujourd'hui, et la précision moyenne du modèle. Un graphique en camembert visualise la distribution des risques (faible, moyen, élevé) parmi tous les patients, permettant une compréhension rapide de la situation globale. Un indicateur central affiche le score de risque moyen de réadmission, permettant une évaluation rapide de la situation globale. Des sections supplémentaires présentent les informations système (version API, modèle ML utilisé, précision) et une liste des patients à risque élevé nécessitant une intervention.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{images/frontend/dashboard.png}
\caption{Dashboard administrateur - Vue d'ensemble du système avec statistiques et graphiques}
\label{fig:frontend-dashboard}
\end{figure}

\subsubsection{Liste des Patients}

La figure \ref{fig:frontend-patients} présente la page de gestion des patients, qui constitue le cœur de l'interface d'administration. Cette page affiche la liste complète de tous les patients enregistrés dans le système, avec leurs informations principales et leurs scores de risque. Des filtres permettent d'afficher tous les patients, uniquement ceux avec un score de risque calculé, ou uniquement ceux sans score encore. Pour chaque patient, l'interface affiche un identifiant, les informations démographiques (âge, genre), le score de risque avec un code couleur (rouge pour élevé, jaune pour moyen, vert pour faible), et la date de dernière prédiction. Les administrateurs et cliniciens peuvent créer de nouveaux patients via le bouton "Nouveau patient" ou accéder au workflow complet via "Workflow Complet". Un bouton de suppression permet de supprimer des patients (avec confirmation via toast), et un bouton "Analyser" permet de lancer le processus d'analyse pour les patients sans score.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{images/frontend/patients.png}
\caption{Liste des patients - Gestion complète avec filtres et actions}
\label{fig:frontend-patients}
\end{figure}

\subsubsection{Workflow Complet du Patient}

La figure \ref{fig:frontend-workflow} présente le workflow complet guidé pour le traitement d'un patient de bout en bout. Cette interface innovante guide l'utilisateur à travers toutes les étapes nécessaires pour créer un patient et obtenir son score de risque. Le workflow est visualisé avec un composant stepper qui montre clairement la progression à travers les 7 étapes: création du patient, ajout d'une hospitalisation, ajout d'observations (signes vitaux et résultats de laboratoire), ajout de conditions (diagnostics), anonymisation, extraction de features, et prédiction du risque. Chaque étape présente un formulaire dédié avec validation, et des boutons "Remplir aléatoirement" permettent de générer des données de test rapidement. Le workflow gère automatiquement les dépendances entre les étapes et synchronise les données entre les services, offrant une expérience utilisateur fluide et guidée.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{images/frontend/workflow.png}
\caption{Workflow complet - Processus guidé étape par étape avec stepper visuel}
\label{fig:frontend-workflow}
\end{figure}

\subsubsection{Détails d'un Patient}

La figure \ref{fig:frontend-patient-detail} présente la page de détails d'un patient, qui affiche toutes les informations complètes concernant un patient spécifique. Cette page regroupe les informations démographiques, l'historique médical complet (hospitalisations, observations, conditions), les features extraites par le service Featurizer, le score de risque calculé avec son niveau (HIGH, MEDIUM, LOW), et les explications SHAP détaillées. Les explications SHAP sont présentées sous forme de liste ordonnée montrant les facteurs contribuant le plus au score de risque, avec des valeurs positives (augmentant le risque) et négatives (diminuant le risque). Cette transparence permet aux cliniciens de comprendre pourquoi un patient a reçu un score de risque spécifique, facilitant ainsi la prise de décision clinique et renforçant la confiance dans le système.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{images/frontend/patient-detail.png}
\caption{Détails d'un patient - Informations complètes et explications SHAP détaillées}
\label{fig:frontend-patient-detail}
\end{figure}

\subsubsection{Page des Prédictions}

La figure \ref{fig:frontend-predictions} présente la page de visualisation des prédictions, qui permet une analyse agrégée de toutes les prédictions effectuées par le système. Cette page affiche des graphiques et tableaux permettant d'analyser les tendances, la distribution des scores de risque, et les performances du modèle. Des filtres permettent de sélectionner des périodes spécifiques, des niveaux de risque, ou des groupes démographiques. Cette interface est particulièrement utile pour les administrateurs et les chercheurs qui souhaitent analyser les performances globales du système et identifier des patterns dans les données, facilitant ainsi l'amélioration continue du modèle et la compréhension des facteurs de risque.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{images/frontend/predictions.png}
\caption{Page des prédictions - Analyse agrégée avec graphiques et tableaux}
\label{fig:frontend-predictions}
\end{figure}

\newpage

% ============================================
% DIAGRAMMES BPMN GLOBAUX
% ============================================
\chapter{Diagrammes BPMN - Processus Métier}

\section{Workflow Complet du Système}

Le diagramme BPMN global (figure \ref{fig:bpmn-global}) représente l'intégration complète de tous les microservices dans le processus métier de prédiction de réadmission.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{images/bpmn/global-workflow-bpmn.png}
\caption{Diagramme BPMN Global - Workflow Complet}
\label{fig:bpmn-global}
\end{figure}

\section{Description du Workflow Global}

Cette section détaille le workflow complet de bout en bout, depuis la création d'un patient jusqu'à la visualisation des résultats, en expliquant comment chaque étape s'intègre dans le processus global.

Le workflow complet suit une séquence logique de dix étapes qui transforment progressivement les données médicales brutes en prédictions exploitables. La première étape consiste en la création du patient dans HAPI FHIR via ProxyFHIR, qui agit comme un proxy pour contourner les restrictions CORS. L'ajout de données médicales complète ensuite le profil du patient avec des Encounters (hospitalisations), Observations (signes vitaux et résultats de laboratoire), et Conditions (diagnostics).

La synchronisation automatique par ProxyFHIR transfère toutes ces données vers PostgreSQL, créant une copie locale pour améliorer les performances. L'anonymisation par le service DeID transforme ensuite les données identifiables en données pseudonymisées selon HIPAA Safe Harbor, générant un pseudo-ID unique pour le patient.

L'extraction de features par Featurizer analyse les données anonymisées et extrait plus de 30 caractéristiques pertinentes, incluant l'analyse NLP des notes cliniques avec BioBERT et spaCy. La prédiction par ModelRisque utilise ces features pour calculer le score de risque de réadmission à 30 jours avec le modèle XGBoost. En parallèle, SHAP génère des explications détaillées qui identifient les facteurs contribuant le plus au score.

ScoreAPI expose ensuite ces résultats via une API REST sécurisée avec authentification JWT, permettant aux applications clientes d'accéder aux prédictions et explications. La visualisation dans le Frontend et AuditFairness présente ces données sous forme de tableaux de bord interactifs, graphiques et métriques de fairness. Enfin, toutes les actions sont automatiquement loggées dans la table d'audit pour garantir la traçabilité et la conformité réglementaire.

\newpage

% ============================================
% TECHNOLOGIES ET DÉPENDANCES
% ============================================
\chapter{Technologies et Dépendances}

\section{Stack Technologique}

\subsection{Backend}

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{Service} & \textbf{Langage} & \textbf{Framework/Technologies} \\
\hline
ProxyFHIR & Java 17 & Spring Boot 3.2, HAPI FHIR 6.10 \\
DeID & Python 3.11 & FastAPI 0.109, SQLAlchemy 2.0 \\
Featurizer & Python 3.11 & FastAPI, BioBERT, spaCy 3.7 \\
ModelRisque & Python 3.11 & FastAPI, XGBoost 2.0, SHAP 0.44 \\
ScoreAPI & Python 3.11 & FastAPI, JWT, bcrypt \\
AuditFairness & Python 3.11 & Dash 2.14, EvidentlyAI 0.4 \\
\hline
\end{tabular}
\caption{Stack technologique backend}
\end{table}

\subsection{Frontend}

\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Technologie} & \textbf{Version} \\
\hline
React & 18.2.0 \\
TypeScript & 5.2.2 \\
Vite & 5.0.8 \\
Tailwind CSS & Latest \\
Recharts & 2.10.3 \\
Axios & 1.6.2 \\
\hline
\end{tabular}
\caption{Stack technologique frontend}
\end{table}

\subsection{Infrastructure}

\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Composant} & \textbf{Version} \\
\hline
Docker & 20.10+ \\
Docker Compose & 2.0+ \\
PostgreSQL & 15-alpine \\
HAPI FHIR & Latest \\
Nginx & Alpine (production) \\
\hline
\end{tabular}
\caption{Infrastructure}
\end{table}

\section{Dépendances Principales}

\subsection{Java (ProxyFHIR)}

\begin{lstlisting}[language=xml, caption=Dépendances Java principales]
<dependencies>
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-web</artifactId>
        <version>3.2.0</version>
    </dependency>
    <dependency>
        <groupId>ca.uhn.hapi.fhir</groupId>
        <artifactId>hapi-fhir-client</artifactId>
        <version>6.10.0</version>
    </dependency>
    <dependency>
        <groupId>org.postgresql</groupId>
        <artifactId>postgresql</artifactId>
    </dependency>
</dependencies>
\end{lstlisting}

\subsection{Python (Services)}

Les services Python partagent plusieurs dépendances communes:

\begin{lstlisting}[language=python, caption=Dépendances Python principales]
fastapi==0.109.0
uvicorn[standard]==0.27.0
sqlalchemy==2.0.25
psycopg2-binary==2.9.9
pydantic==2.5.3
numpy==1.26.3
pandas==2.1.4
\end{lstlisting}

\subsection{Intelligence Artificielle}

\begin{lstlisting}[language=python, caption=Dépendances IA/ML]
# Machine Learning
xgboost==2.0.3
shap==0.44.1
scikit-learn==1.4.0

# NLP
spacy==3.7.2
transformers==4.36.2
torch==2.1.2

# Fairness
evidently==0.4.33
\end{lstlisting}

\newpage

% ============================================
% BASE DE DONNÉES
% ============================================
\chapter{Base de Données}

\section{Schéma de Base de Données}

Cette section présente l'organisation de la base de données PostgreSQL qui centralise toutes les données du système HealthFlow-MS.

La base de données PostgreSQL centralise toutes les données du système, servant de référentiel unique pour les données FHIR synchronisées, les patients anonymisés, les caractéristiques extraites, les prédictions de risque, et les logs d'audit. Le schéma est organisé en plusieurs groupes de tables principales qui reflètent les différentes étapes du pipeline de traitement.

Le premier groupe comprend les tables \texttt{fhir\_*} qui stockent les données brutes synchronisées depuis HAPI FHIR: \texttt{fhir\_patients} contient les informations démographiques des patients, \texttt{fhir\_encounters} stocke les hospitalisations, \texttt{fhir\_observations} regroupe les signes vitaux et résultats de laboratoire, et \texttt{fhir\_conditions} contient les diagnostics et conditions médicales.

Le deuxième groupe concerne les données anonymisées: \texttt{deid\_patients} stocke les patients après anonymisation selon HIPAA Safe Harbor, avec un pseudo-ID unique qui remplace l'identifiant FHIR original.

Le troisième groupe regroupe les données dérivées pour le Machine Learning: \texttt{patient\_features} contient les 30+ caractéristiques extraites par le service Featurizer, et \texttt{risk\_predictions} stocke les scores de risque calculés par ModelRisque ainsi que les explications SHAP.

Enfin, les tables de gestion et d'audit incluent \texttt{users} pour les utilisateurs du système avec leurs rôles et permissions, \texttt{ml\_models} pour les métadonnées des modèles ML (version, métriques, date d'entraînement), et \texttt{api\_audit\_logs} pour tracer toutes les actions effectuées dans le système.

\section{Diagramme Entité-Relation}

La figure \ref{fig:er-diagram} présente le diagramme entité-relation de la base de données.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{images/database/er-diagram.png}
\caption{Diagramme Entité-Relation}
\label{fig:er-diagram}
\end{figure}

\section{Relations Principales}

Les relations entre les tables sont organisées selon un modèle hiérarchique qui reflète le flux de données du système. La relation principale lie \texttt{deid\_patients.original\_fhir\_id} à \texttt{fhir\_patients.fhir\_id}, créant un lien entre les données identifiables et anonymisées. Cette relation permet de tracer l'origine des données anonymisées tout en préservant la confidentialité.

Les tables de features et de prédictions sont liées aux patients anonymisés via \texttt{patient\_features.pseudo\_patient\_id} et \texttt{risk\_predictions.pseudo\_patient\_id}, qui référencent tous deux \texttt{deid\_patients.pseudo\_id}. Cette architecture garantit que seules les données anonymisées sont utilisées pour l'analyse et la prédiction, respectant ainsi les normes de confidentialité.

Les données FHIR sont organisées autour du patient comme entité centrale: \texttt{fhir\_encounters.patient\_fhir\_id}, \texttt{fhir\_observations.patient\_fhir\_id}, et \texttt{fhir\_conditions.patient\_fhir\_id} référencent tous \texttt{fhir\_patients.fhir\_id}, créant une structure relationnelle cohérente qui permet de regrouper toutes les informations médicales d'un patient.

\section{Index et Performance}

Pour optimiser les performances des requêtes fréquentes, des index ont été créés stratégiquement sur les colonnes les plus souvent utilisées dans les jointures et les filtres.

Les index principaux incluent un index sur \texttt{fhir\_patients.fhir\_id} pour accélérer les recherches de patients par leur identifiant FHIR, un index sur \texttt{deid\_patients.pseudo\_id} pour les recherches de patients anonymisés, et un index sur \texttt{deid\_patients.original\_fhir\_id} pour faciliter la traçabilité entre données identifiables et anonymisées. Les tables de features et de prédictions bénéficient également d'index sur leurs clés étrangères (\texttt{patient\_features.pseudo\_patient\_id} et \texttt{risk\_predictions.pseudo\_patient\_id}) pour accélérer les jointures. Un index supplémentaire sur \texttt{risk\_predictions.prediction\_timestamp} permet de trier rapidement les prédictions par date, facilitant ainsi les analyses temporelles et les rapports de performance.

\newpage

% ============================================
% INTELLIGENCE ARTIFICIELLE
% ============================================
\chapter{Intelligence Artificielle et Machine Learning}

\section{Modèle de Prédiction - XGBoost}

\subsection{Présentation et Philosophie}

Dans le domaine de la prédiction médicale, où chaque décision peut avoir des conséquences significatives sur la vie des patients, le choix de l'algorithme de Machine Learning n'est pas anodin. HealthFlow-MS s'appuie sur XGBoost (Extreme Gradient Boosting), un algorithme d'ensemble sophistiqué qui représente l'état de l'art en matière d'apprentissage supervisé pour les problèmes de classification. Contrairement aux modèles linéaires traditionnels qui peinent à capturer les interactions complexes entre les variables médicales, XGBoost construit une forêt d'arbres de décision qui apprennent progressivement à identifier les patterns subtils dans les données cliniques. Chaque arbre de la forêt corrige les erreurs de ses prédécesseurs, créant ainsi un modèle collectif dont la sagesse dépasse largement celle de ses composants individuels. Cette approche d'ensemble, inspirée du principe démocratique où la majorité l'emporte, permet de réduire significativement le risque de surapprentissage tout en maintenant une capacité prédictive exceptionnelle.

L'architecture de notre modèle XGBoost a été méticuleusement calibrée pour équilibrer précision et interprétabilité. Le modèle utilise un ensemble de 500 arbres de décision, chacun limité à une profondeur maximale de 6 niveaux. Cette limitation de profondeur n'est pas une contrainte mais un choix stratégique : elle force le modèle à apprendre des règles généralisables plutôt que de mémoriser des cas spécifiques, garantissant ainsi une meilleure performance sur de nouveaux patients non vus pendant l'entraînement. Le taux d'apprentissage de 0.05, relativement conservateur, permet à chaque arbre d'apporter des corrections subtiles et progressives, évitant les ajustements brusques qui pourraient déstabiliser le modèle. Le mécanisme de subsampling, qui n'utilise que 80\% des données pour entraîner chaque arbre, introduit une diversité artificielle qui renforce la robustesse de l'ensemble. Enfin, la régularisation L1 (alpha=0.01) et L2 (lambda=1) agit comme un garde-fou statistique, pénalisant les modèles trop complexes et favorisant les solutions parcimonieuses qui généralisent mieux.

\subsection{Ingénierie des Features : Une Symphonie de Données}

Le succès d'un modèle de Machine Learning ne réside pas uniquement dans la sophistication de l'algorithme, mais également dans la qualité et la pertinence des features qui lui sont fournies. HealthFlow-MS extrait et prépare plus de 30 caractéristiques cliniques soigneusement sélectionnées, chacune apportant une perspective unique sur l'état de santé du patient. Ces features ne sont pas simplement des variables numériques extraites mécaniquement des bases de données ; elles représentent une synthèse intelligente de connaissances médicales, transformant des données brutes en signaux prédictifs significatifs.

Les features démographiques, telles que l'âge à l'admission et le genre encodé, capturent les facteurs de risque intrinsèques liés au profil biologique du patient. L'âge, par exemple, n'est pas simplement un nombre mais un proxy pour la fragilité physiologique, la capacité de récupération, et l'accumulation de comorbidités au fil du temps. Les features cliniques temporelles, incluant la durée de séjour et l'historique des hospitalisations précédentes (30 jours, 90 jours, 365 jours), révèlent des patterns de récurrence et de chronicité qui sont des indicateurs puissants du risque de réadmission. Un patient avec plusieurs hospitalisations récentes suggère une instabilité médicale sous-jacente que le modèle apprend à reconnaître.

Les indices de comorbidité, notamment l'index de Charlson et le score d'Elixhauser, condensent des décennies de recherche épidémiologique en un seul nombre. Ces scores ne sont pas de simples additions de maladies ; ils reflètent la complexité médicale, la charge de morbidité, et l'interaction synergique entre différentes conditions pathologiques. Un patient avec un index de Charlson élevé n'est pas simplement "plus malade" ; il représente un cas médicalement complexe où les interactions entre maladies créent un risque exponentiel plutôt qu'additif.

Les signes vitaux et les valeurs de laboratoire, capturés au moment de l'admission ou juste avant la sortie, offrent un instantané physiologique de l'état du patient. Une créatinine élevée suggère une insuffisance rénale qui peut compromettre l'élimination des médicaments et augmenter le risque de complications. Une hémoglobine basse indique une anémie qui peut limiter la capacité de récupération. Ces mesures ne sont pas statiques mais dynamiques, et le modèle apprend à interpréter leurs valeurs dans le contexte global du patient.

Enfin, les features NLP, extraites des notes cliniques non structurées, apportent une dimension qualitative unique. Le score de sentiment révèle le ton émotionnel des notes, où un sentiment négatif peut indiquer une préoccupation clinique sous-jacente. Le score d'urgence quantifie le niveau d'inquiétude perçu par les cliniciens, tandis que le score de complexité capture la sophistication médicale requise pour gérer le cas. Le nombre d'entités médicales extraites (médicaments, symptômes, maladies) reflète la richesse informationnelle des notes, où plus d'informations peuvent paradoxalement indiquer soit un cas complexe soit une documentation exhaustive.

\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Catégorie} & \textbf{Features} \\
\hline
Démographiques & age\_at\_admission, gender\_encoded \\
Cliniques & length\_of\_stay, previous\_admissions\_* \\
Comorbidités & charlson\_index, elixhauser\_score \\
Signes vitaux & heart\_rate, blood\_pressure, temperature \\
Laboratoire & hemoglobin, creatinine, glucose \\
NLP & sentiment\_score, urgency\_score, entities\_count \\
\hline
\end{tabular}
\caption{Catégories de features}
\end{table}

\subsection{Processus d'Entraînement et Optimisation}

L'entraînement du modèle XGBoost dans HealthFlow-MS suit un processus rigoureux qui garantit à la fois la performance prédictive et la généralisation à de nouveaux patients. Le processus commence par la préparation des données, où les features sont normalisées et les valeurs manquantes sont imputées de manière appropriée. Contrairement aux approches naïves qui remplacent simplement les valeurs manquantes par zéro ou la moyenne, notre système utilise des stratégies d'imputation contextuelles qui préservent la signification médicale des données. Par exemple, une valeur manquante pour la créatinine chez un patient sans historique de maladie rénale est traitée différemment d'une valeur manquante chez un patient avec insuffisance rénale chronique.

L'entraînement utilise une validation croisée stratifiée pour garantir que le modèle est évalué sur des distributions de patients représentatives. Cette approche est cruciale en médecine, où les classes peuvent être déséquilibrées (moins de patients à haut risque que de patients à faible risque). La stratification garantit que chaque fold de validation contient une proportion similaire de patients à haut risque, permettant ainsi une évaluation robuste de la performance du modèle sur les cas les plus critiques. Le processus d'optimisation des hyperparamètres utilise une recherche bayésienne plutôt qu'une grille exhaustive, permettant d'explorer efficacement l'espace des hyperparamètres tout en évitant le surapprentissage.

Le modèle est évalué non seulement sur des métriques globales comme l'AUC-ROC (Area Under the Receiver Operating Characteristic curve), qui atteint 0.82 dans notre système, mais également sur des métriques cliniquement pertinentes comme la sensibilité à différents seuils de risque. Une sensibilité élevée est cruciale pour identifier les patients à haut risque, même au prix d'un taux de faux positifs légèrement plus élevé. Le modèle est également évalué sur sa calibration, garantissant que lorsque le modèle prédit un risque de 70\%, environ 70\% des patients sont effectivement réadmis, ce qui est essentiel pour la confiance clinique.

\section{Data Mining et Traitement du Langage Naturel (NLP)}

\subsection{Introduction : Transformer le Texte en Intelligence}

Dans le domaine médical, une quantité considérable d'informations précieuses réside non pas dans des champs structurés de bases de données, mais dans les notes cliniques libres rédigées par les médecins, les infirmières et autres professionnels de santé. Ces notes, souvent rédigées sous la pression du temps et dans un langage naturel riche en nuances, contiennent des indices subtils sur l'état du patient, le niveau d'inquiétude des cliniciens, et la complexité du cas. Le défi du Data Mining dans HealthFlow-MS consiste à extraire cette intelligence latente des textes non structurés et à la transformer en features quantitatives exploitables par les modèles de Machine Learning. Cette transformation n'est pas une simple conversion mécanique mais un processus sophistiqué qui combine les dernières avancées en traitement du langage naturel avec une compréhension profonde du domaine médical.

\subsection{BioBERT : La Compréhension Sémantique Biomédicale}

BioBERT représente une révolution dans la compréhension automatique du langage médical. Contrairement aux modèles de traitement du langage naturel génériques qui sont entraînés sur des corpus de textes généralistes (articles de presse, livres, pages web), BioBERT a été spécifiquement pré-entraîné sur des millions de documents biomédicaux, incluant des articles PubMed, des résumés de recherche clinique, et des textes médicaux spécialisés. Cette spécialisation n'est pas cosmétique mais fondamentale : le langage médical possède sa propre syntaxe, son propre vocabulaire, et ses propres règles sémantiques. Un mot comme "dyspnée" dans un contexte médical n'a pas la même signification que dans un texte général, et BioBERT a appris ces subtilités à travers des milliards de paramètres ajustés sur des corpus biomédicaux.

Lorsqu'une note clinique entre dans le système, BioBERT la transforme en un vecteur d'embeddings de 768 dimensions, où chaque dimension capture un aspect différent du sens sémantique du texte. Ces embeddings ne sont pas de simples représentations numériques mais des encodages profonds qui capturent les relations conceptuelles entre les mots, les phrases, et les concepts médicaux. Par exemple, BioBERT comprend que "insuffisance cardiaque" et "défaillance myocardique" sont conceptuellement similaires, même si les mots utilisés sont différents. Cette compréhension sémantique permet au système de calculer des scores de complexité et d'urgence qui reflètent non pas simplement la fréquence de certains mots-clés, mais la signification médicale profonde du texte.

Le processus d'extraction avec BioBERT fonctionne en plusieurs étapes. D'abord, le texte est tokenisé en unités significatives (mots, sous-mots, caractères spéciaux) selon le vocabulaire médical appris. Ensuite, chaque token est transformé en embedding contextuel, où le même mot peut avoir des représentations différentes selon son contexte. Par exemple, "hypertension" dans "hypertension artérielle" aura un embedding différent de "hypertension" dans "hypertension intracrânienne", reflétant ainsi la polysémie médicale. Ces embeddings contextuels sont ensuite agrégés pour créer une représentation globale du document, qui capture à la fois le contenu factuel (diagnostics, médicaments, symptômes) et le contexte émotionnel et clinique (niveau d'inquiétude, complexité perçue).

\subsection{spaCy : L'Extraction Précise d'Entités Médicales}

Alors que BioBERT excelle dans la compréhension sémantique globale, spaCy apporte une précision chirurgicale dans l'extraction d'entités nommées médicales. La reconnaissance d'entités nommées (NER) dans le domaine médical est particulièrement complexe car les entités médicales peuvent avoir des formes variées : un médicament peut être mentionné par son nom générique, son nom commercial, ou même par son code ATC. Un symptôme peut être décrit de manière explicite ("douleur thoracique") ou implicite ("gêne rétrosternale"). spaCy, avec ses modèles pré-entraînés et ses capacités d'analyse syntaxique, identifie et extrait ces entités avec une précision remarquable.

Le processus d'extraction avec spaCy commence par une analyse syntaxique approfondie du texte. Chaque phrase est parsée pour identifier les relations grammaticales entre les mots, permettant ainsi de distinguer entre "le patient prend de la metformine" (médicament actif) et "le patient a arrêté la metformine" (médicament discontinué). Cette compréhension syntaxique est cruciale car elle permet d'extraire non seulement les entités mais aussi leurs relations et leurs états. spaCy identifie également les patterns médicaux complexes, comme les dosages de médicaments ("500mg deux fois par jour"), les valeurs de laboratoire ("créatinine à 1.8 mg/dL"), et les dates cliniques ("hospitalisé le 15 mars").

Pour compléter l'extraction basée sur les modèles, HealthFlow-MS utilise également des patterns regex spécialisés pour capturer des entités médicales qui suivent des formats standardisés. Les codes ICD-10, les codes LOINC, les noms de médicaments avec leurs dosages, et les valeurs numériques avec leurs unités sont tous identifiés grâce à des patterns soigneusement conçus. Cette approche hybride, combinant l'intelligence des modèles de langage avec la précision des patterns structurés, garantit une extraction exhaustive et précise des informations médicales.

\subsection{Les Features NLP : De la Sémantique à la Prédiction}

Les features NLP extraites par HealthFlow-MS ne sont pas de simples métriques textuelles mais des indicateurs cliniques sophistiqués qui capturent différentes dimensions de l'information médicale. Le score de sentiment, calculé en analysant le ton émotionnel et la polarité des notes cliniques, révèle le niveau d'inquiétude ou d'optimisme des cliniciens. Une note avec un sentiment très négatif peut indiquer une préoccupation clinique sérieuse, même si les valeurs numériques (signes vitaux, laboratoires) semblent normales. Ce score est calculé en analysant les embeddings BioBERT et en les projetant sur une dimension sentimentale apprise à partir de corpus médicaux annotés.

Le score d'urgence quantifie le niveau d'urgence perçu dans les notes cliniques. Contrairement au triage traditionnel qui se base sur des critères objectifs, ce score capture l'urgence subjective perçue par les cliniciens, qui peut être influencée par des facteurs subtils non capturés par les données structurées. Des mots-clés comme "urgent", "critique", "décompensation", "instable" contribuent positivement au score, tandis que des termes comme "stable", "amélioration", "contrôle" le diminuent. Ce score est particulièrement utile car il reflète l'expertise clinique et l'intuition médicale, qui sont difficiles à quantifier autrement.

Le score de complexité mesure la sophistication médicale requise pour gérer le cas. Un cas complexe n'est pas nécessairement un cas grave, mais un cas qui nécessite une expertise spécialisée, une coordination multi-disciplinaire, ou une gestion nuancée. Ce score est calculé en analysant la diversité des entités médicales mentionnées, la longueur et la structure des notes, et la présence de termes techniques spécialisés. Un patient avec un score de complexité élevé peut nécessiter des ressources supplémentaires et un suivi plus intensif, même si son risque de réadmission n'est pas nécessairement le plus élevé.

Les métriques d'entités (entities\_count, medication\_mentions, symptom\_mentions) quantifient la richesse informationnelle des notes. Un nombre élevé d'entités extraites peut indiquer soit un cas médicalement riche avec de nombreuses conditions et traitements, soit une documentation exhaustive. Ces métriques, combinées avec les scores sémantiques, créent un profil complet de l'information textuelle disponible pour chaque patient, permettant au modèle de Machine Learning d'exploiter pleinement cette source d'information précieuse.

Le \textbf{sentiment\_score} quantifie le sentiment général de la note clinique sur une échelle de 0 à 1, reflétant le ton émotionnel et le niveau d'inquiétude des cliniciens. Ce score est calculé en analysant les embeddings BioBERT et en les projetant sur une dimension sentimentale apprise à partir de corpus médicaux annotés. Un score proche de 0 indique un sentiment très négatif, suggérant une préoccupation clinique sérieuse, tandis qu'un score proche de 1 indique un sentiment positif, suggérant une situation clinique stable ou en amélioration. Ce score capture des nuances subtiles qui ne sont pas visibles dans les données structurées : un patient avec des signes vitaux normaux mais un sentiment très négatif dans les notes peut indiquer une préoccupation clinique sous-jacente que les valeurs numériques ne révèlent pas.

Le \textbf{urgency\_score} quantifie le niveau d'urgence perçu dans les notes cliniques sur une échelle de 0 à 1, capturant l'urgence subjective basée sur l'expertise clinique plutôt que sur des critères objectifs standardisés. Ce score est calculé en analysant la présence de mots-clés d'urgence ("urgent", "critique", "décompensation", "instable") et en pondérant leur importance selon leur contexte. Contrairement au triage traditionnel qui se base sur des critères objectifs, ce score capture l'urgence perçue par les cliniciens, qui peut être influencée par des facteurs subtils non capturés par les données structurées. Des mots-clés comme "stable", "amélioration", "contrôle" contribuent négativement au score, tandis que des termes comme "instable", "décompensation", "critique" contribuent positivement. Ce score est particulièrement utile car il reflète l'expertise clinique et l'intuition médicale, qui sont difficiles à quantifier autrement.

Le \textbf{complexity\_score} mesure la sophistication médicale requise pour gérer le cas sur une échelle de 0 à 1, quantifiant la complexité du cas médical. Un cas complexe n'est pas nécessairement un cas grave, mais un cas qui nécessite une expertise spécialisée, une coordination multi-disciplinaire, ou une gestion nuancée. Ce score est calculé en analysant la diversité des entités médicales mentionnées, la longueur et la structure des notes, et la présence de termes techniques spécialisés. Un patient avec un score de complexité élevé peut nécessiter des ressources supplémentaires et un suivi plus intensif, même si son risque de réadmission n'est pas nécessairement le plus élevé. Ce score permet d'identifier les cas qui nécessitent une attention particulière et des ressources spécialisées, facilitant ainsi la planification des soins et l'allocation des ressources.

Le \textbf{entities\_count} quantifie le nombre total d'entités médicales extraites des notes cliniques, incluant les médicaments, symptômes, maladies, et procédures. Ce comptage reflète la richesse informationnelle des notes : un nombre élevé d'entités peut indiquer soit un cas médicalement riche avec de nombreuses conditions et traitements, soit une documentation exhaustive. Cette métrique est particulièrement utile car elle capture la densité informationnelle des notes, qui peut être un indicateur de la complexité du cas ou de la qualité de la documentation. Les entités sont extraites à la fois par spaCy (pour les entités générales) et par des patterns regex spécialisés (pour les codes standardisés comme ICD-10 et LOINC), garantissant ainsi une extraction exhaustive.

Le \textbf{medication\_mentions} compte le nombre de mentions de médicaments dans les notes cliniques, incluant à la fois les médicaments actifs et passés. Cette métrique est importante car elle reflète la charge médicamenteuse du patient, qui peut être un facteur de risque significatif pour la réadmission. Un nombre élevé de médicaments peut indiquer soit une polypharmacie (plusieurs médicaments simultanés), soit un historique médical complexe avec de nombreux traitements passés. Cette distinction est importante car la polypharmacie actuelle peut être un facteur de risque pour les interactions médicamenteuses et les effets secondaires, tandis qu'un historique de nombreux traitements passés peut indiquer une maladie chronique complexe. Le système distingue entre les médicaments actifs et passés grâce à l'analyse syntaxique de spaCy, permettant ainsi une analyse plus nuancée.

Enfin, le \textbf{symptom\_mentions} compte le nombre de mentions de symptômes dans les notes cliniques, capturant la présentation clinique du patient. Cette métrique reflète la richesse de la présentation symptomatique : un patient avec de nombreux symptômes peut avoir une présentation complexe qui nécessite une évaluation approfondie, tandis qu'un patient avec peu de symptômes peut avoir une présentation plus simple. Cette métrique est particulièrement utile car elle capture des informations qui ne sont pas toujours codées dans les données structurées : les symptômes peuvent être mentionnés dans les notes cliniques sans être formellement codés comme des diagnostics. Le comptage des symptômes permet ainsi d'exploiter cette information précieuse qui serait autrement perdue.

\section{Explicabilité - SHAP : Rendre le Modèle Transparent}

\subsection{La Nécessité de l'Explicabilité en Médecine}

Dans le domaine médical, où chaque décision peut avoir des conséquences vitales, la confiance dans les systèmes d'intelligence artificielle ne peut pas être basée uniquement sur la performance statistique. Les cliniciens doivent comprendre non seulement \textit{quoi} le modèle prédit, mais également \textit{pourquoi} il fait cette prédiction. Cette exigence d'explicabilité n'est pas un luxe académique mais une nécessité pratique : un médecin ne peut pas prendre une décision clinique basée sur un score de risque opaque, même si ce score est statistiquement précis. HealthFlow-MS répond à ce défi en intégrant SHAP (SHapley Additive exPlanations), une méthode d'explication basée sur la théorie des jeux coopératifs qui fournit des explications mathématiquement rigoureuses et cliniquement interprétables.

SHAP s'inspire de la théorie de Shapley, développée initialement pour résoudre le problème de répartition équitable des gains dans les jeux coopératifs. Dans le contexte de la prédiction médicale, chaque feature (variable clinique) peut être considérée comme un "joueur" dans un "jeu" où l'objectif est de prédire le risque de réadmission. La valeur SHAP d'une feature représente sa contribution marginale moyenne à la prédiction, calculée en évaluant l'impact de cette feature dans toutes les combinaisons possibles avec les autres features. Cette approche garantit une attribution équitable et cohérente de la contribution, où la somme des valeurs SHAP de toutes les features égale exactement la différence entre la prédiction du patient et la prédiction moyenne de la population.

\subsection{Interprétation Clinique des Valeurs SHAP}

L'interprétation des valeurs SHAP est intuitive et puissante pour les cliniciens. Une valeur SHAP positive indique que la feature augmente le risque de réadmission par rapport à la moyenne de la population. Par exemple, si un patient a un nombre élevé d'hospitalisations précédentes dans les 30 derniers jours, cette feature aura une valeur SHAP positive significative, indiquant que l'historique récent d'hospitalisations est un facteur de risque majeur pour ce patient spécifique. Inversement, une valeur SHAP négative indique que la feature diminue le risque. Un patient avec des signes vitaux stables et normaux aura des valeurs SHAP négatives pour ces features, suggérant que son état physiologique actuel est protecteur contre la réadmission.

La magnitude de la valeur SHAP reflète l'importance relative de la contribution. Une valeur SHAP de +0.15 pour "previous\_admissions\_30d" est plus significative qu'une valeur de +0.05 pour "age\_at\_admission", indiquant que l'historique récent d'hospitalisations est un facteur de risque plus important que l'âge pour ce patient spécifique. Cette hiérarchisation des facteurs de risque permet aux cliniciens de se concentrer sur les aspects les plus critiques du cas du patient, optimisant ainsi l'utilisation de leur temps et de leurs ressources limitées.

L'une des forces de SHAP est sa capacité à capturer les interactions entre features. Contrairement aux modèles linéaires qui supposent des effets indépendants, XGBoost peut apprendre des interactions complexes, et SHAP révèle ces interactions de manière interprétable. Par exemple, un patient âgé avec une créatinine élevée peut avoir une valeur SHAP plus élevée pour cette combinaison que la somme des valeurs SHAP individuelles, révélant une synergie de risque qui ne serait pas apparente dans une analyse univariée.

\subsection{Exemple d'Explication}

Pour un patient avec un score de risque de 0.78:

\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Feature} & \textbf{Contribution SHAP} \\
\hline
previous\_admissions\_30d & +0.15 \\
charlson\_comorbidity\_index & +0.12 \\
creatinine\_last & +0.08 \\
age\_at\_admission & +0.05 \\
hemoglobin\_last & -0.03 \\
\hline
\end{tabular}
\caption{Exemple de valeurs SHAP}
\end{table}

\section{Fairness et Biais}

\subsection{Métriques Calculées}

Le service AuditFairness calcule plusieurs métriques statistiques pour détecter et quantifier les biais potentiels dans les prédictions du modèle, garantissant ainsi l'équité et la non-discrimination.

La métrique de Demographic Parity (égalité démographique) mesure le ratio des taux de prédiction positive entre différents groupes démographiques (par exemple, hommes versus femmes, différents groupes d'âge). Un ratio proche de 1 indique que le modèle prédit des risques similaires pour tous les groupes, tandis qu'un ratio éloigné de 1 suggère un biais potentiel. La métrique d'Equalized Odds (chances égales) vérifie la parité des taux de vrais positifs et de faux positifs entre les groupes, garantissant que le modèle a la même précision pour tous les groupes démographiques. La métrique d'Equal Opportunity (égalité des opportunités) se concentre spécifiquement sur la parité des taux de vrais positifs, assurant que les patients à haut risque sont identifiés avec la même précision dans tous les groupes. Enfin, la métrique de Calibration (calibration) vérifie la cohérence des probabilités prédites avec les résultats réels observés, garantissant que lorsque le modèle prédit un risque de 70\%, environ 70\% des patients sont effectivement réadmis.

\subsection{Seuils Acceptables}

\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Métrique} & \textbf{Seuil Acceptable} \\
\hline
Demographic Parity Ratio & > 0.8 \\
Equalized Odds Difference & < 0.1 \\
Equal Opportunity Difference & < 0.1 \\
\hline
\end{tabular}
\caption{Seuils de fairness}
\end{table}

\newpage

% ============================================
% SÉCURITÉ ET CONFORMITÉ
% ============================================
\chapter{Sécurité et Conformité}

\section{Authentification et Autorisation}

\subsection{JWT (JSON Web Tokens)}

L'authentification dans HealthFlow-MS repose sur le standard JWT (JSON Web Tokens), une solution moderne et stateless qui évite de maintenir des sessions serveur.

Le système utilise deux types de tokens JWT pour équilibrer sécurité et expérience utilisateur. L'Access Token est valide pendant 24 heures, permettant aux utilisateurs de rester connectés pendant une journée de travail sans avoir à se ré-authentifier fréquemment. Le Refresh Token est valide pendant 7 jours, permettant de renouveler l'Access Token sans demander à l'utilisateur de saisir à nouveau ses identifiants. La signature utilise l'algorithme HMAC-SHA256, garantissant l'intégrité et l'authenticité du token. L'encodage Base64URL permet une transmission sécurisée dans les en-têtes HTTP et les URLs, tout en restant compatible avec les systèmes web modernes.

\subsection{Rôles et Permissions}

\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Rôle} & \textbf{Permissions} \\
\hline
admin & Toutes les opérations \\
clinician & Créer/modifier/supprimer patients, voir prédictions \\
researcher & Lecture seule (patients, prédictions) \\
auditor & Lecture seule + accès aux logs d'audit \\
\hline
\end{tabular}
\caption{Rôles et permissions}
\end{table}

\section{Anonymisation HIPAA}

\subsection{Standard HIPAA Safe Harbor}

Le service DeID implémente rigoureusement la méthode HIPAA Safe Harbor, une approche standardisée reconnue par la réglementation américaine HIPAA pour l'anonymisation des données de santé.

Cette méthode spécifie précisément 18 identifiants personnels qui doivent être supprimés ou modifiés pour que les données soient considérées comme anonymisées. Une fois cette anonymisation appliquée, les données peuvent être utilisées pour la recherche et l'analyse sans autorisation supplémentaire des patients, car elles ne sont plus considérées comme des informations de santé protégées (PHI - Protected Health Information). Cette approche offre un équilibre optimal entre la protection de la confidentialité et l'utilité des données pour l'analyse, permettant ainsi de mener des recherches médicales importantes tout en respectant les droits des patients.

\subsection{Techniques Utilisées}

Le service DeID applique plusieurs techniques d'anonymisation adaptées au type de données à protéger. La suppression complète est utilisée pour les identifiants directs comme les noms, adresses complètes, numéros de téléphone et adresses email, qui sont entièrement retirés des données. La généralisation transforme les dates précises en années uniquement, préservant ainsi l'utilité des données pour l'analyse temporelle tout en empêchant l'identification par combinaison avec d'autres sources. La pseudonymisation remplace les identifiants uniques (comme les IDs de patients) par des pseudo-IDs générés de manière déterministe, permettant de maintenir la cohérence des données tout en masquant l'identité réelle. Enfin, l'agrégation transforme les valeurs numériques précises (comme l'âge exact) en catégories (groupes d'âge), réduisant ainsi le risque de ré-identification tout en conservant l'information nécessaire pour l'analyse statistique.

\section{Conformité RGPD}

HealthFlow-MS respecte scrupuleusement les principes du Règlement Général sur la Protection des Données (RGPD), garantissant ainsi la conformité avec la législation européenne sur la protection des données personnelles.

Le principe de minimisation des données est appliqué en collectant uniquement les données strictement nécessaires pour les objectifs du système, évitant ainsi la collecte excessive d'informations. L'anonymisation systématique des données personnelles via le service DeID transforme les données identifiables en données pseudonymisées, réduisant ainsi le champ d'application du RGPD. Le droit à l'oubli est implémenté en permettant la suppression complète des données d'un patient sur demande, avec une propagation automatique de cette suppression dans toutes les tables concernées. La transparence est garantie par des logs d'audit complets qui enregistrent toutes les actions effectuées sur les données, permettant ainsi aux utilisateurs de comprendre comment leurs données sont utilisées. Enfin, la sécurité est renforcée par le chiffrement des mots de passe avec l'algorithme bcrypt, qui utilise un coût de calcul élevé pour résister aux attaques par force brute.

\section{Logs d'Audit}

Un système complet de logs d'audit garantit la traçabilité de toutes les actions effectuées dans HealthFlow-MS, répondant ainsi aux exigences de conformité et de sécurité.

Toutes les actions sont automatiquement enregistrées dans la table \texttt{api\_audit\_logs} avec un niveau de détail suffisant pour permettre une analyse forensique complète. Chaque entrée de log inclut le timestamp précis de l'action, permettant de reconstruire la chronologie des événements. L'identité de l'utilisateur ayant effectué l'action est enregistrée, permettant de tracer la responsabilité des opérations. Le type d'action (CREATE, READ, UPDATE, DELETE) est documenté pour comprendre la nature de l'opération. La ressource concernée (patient, prédiction, utilisateur, etc.) est identifiée pour permettre des recherches ciblées. Enfin, les détails complets de la requête (méthode HTTP, endpoint, paramètres) sont conservés pour permettre une analyse approfondie en cas d'incident de sécurité ou de non-conformité.

\newpage

% ============================================
% TESTS ET VALIDATION
% ============================================
\chapter{Tests et Validation}

\section{Stratégie de Tests}

Cette section présente la stratégie de tests complète implémentée pour HealthFlow-MS, couvrant tous les niveaux de test depuis les tests unitaires jusqu'aux tests de performance.

\subsection{Types de Tests}

Le projet implémente une stratégie de tests en pyramide, avec quatre niveaux de tests complémentaires qui garantissent la qualité et la fiabilité du système à tous les niveaux d'abstraction. Cette approche pyramidale est essentielle car elle permet de détecter les erreurs le plus tôt possible dans le cycle de développement, réduisant ainsi les coûts de correction et améliorant la confiance dans le système.

Les \textbf{tests unitaires} constituent la base de la pyramide et vérifient le comportement des fonctions individuelles de chaque service, isolées de leurs dépendances externes. Pour les services Python, pytest est utilisé comme framework de test, offrant une syntaxe claire et expressive pour définir les tests, des fixtures puissantes pour la gestion des données de test, et des rapports détaillés qui facilitent l'identification des problèmes. Pour le service Java (ProxyFHIR), JUnit est utilisé, offrant une intégration native avec l'écosystème Spring et permettant de tester les composants Spring en isolation grâce à l'injection de dépendances de test. Les tests unitaires couvrent toutes les fonctions critiques, des calculs de features aux transformations de données, en passant par la validation des entrées et la gestion des erreurs. Cette couverture exhaustive garantit que chaque composant fonctionne correctement individuellement avant d'être intégré dans le système plus large.

Les \textbf{tests d'intégration} montent d'un niveau dans la pyramide et valident la communication et l'interaction entre les différents microservices, garantissant que les APIs fonctionnent correctement ensemble. Ces tests utilisent pytest avec des requêtes HTTP réelles pour simuler les interactions entre services, testant non seulement que les endpoints répondent correctement, mais également que les données sont correctement transformées et transmises entre les services. Les tests d'intégration vérifient les workflows critiques comme la synchronisation FHIR, l'anonymisation des données, l'extraction de features, et la génération de prédictions, garantissant que l'ensemble du pipeline fonctionne de manière cohérente et fiable.

Les \textbf{tests end-to-end} représentent le niveau supérieur de la pyramide et vérifient le workflow complet depuis la création d'un patient jusqu'à l'affichage des résultats dans le frontend, simulant un usage réel du système. Ces tests utilisent Selenium pour automatiser l'interaction avec le navigateur, permettant de tester l'interface utilisateur de manière réaliste. Les tests end-to-end vérifient non seulement que les fonctionnalités techniques fonctionnent, mais également que l'expérience utilisateur est fluide et intuitive, que les données sont correctement affichées, et que les interactions utilisateur produisent les résultats attendus. Ces tests sont particulièrement importants car ils valident le système du point de vue de l'utilisateur final, garantissant que le système répond aux besoins réels des professionnels de santé.

Enfin, les \textbf{tests de performance} évaluent la charge et la latence du système sous différentes conditions de stress, identifiant les goulots d'étranglement et les limites de scalabilité. Locust est utilisé comme outil de test de charge, permettant de définir des scénarios de charge réalistes et de mesurer les performances du système sous différentes conditions. Ces tests vérifient que le système peut gérer la charge attendue en production, que les temps de réponse restent acceptables même sous charge élevée, et que le système se comporte de manière stable et prévisible. Les tests de performance sont essentiels pour garantir que le système peut être déployé en production avec confiance, sachant qu'il peut gérer la charge réelle sans dégradation significative des performances.

\section{Validation du Modèle ML}

\subsection{Métriques de Performance}

Le modèle a été validé sur un dataset de test avec les résultats suivants:

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{Métrique} & \textbf{Valeur} & \textbf{Seuil} \\
\hline
AUC-ROC & 0.82 & > 0.75 \\
Précision & 0.78 & > 0.70 \\
Rappel & 0.74 & > 0.70 \\
F1-Score & 0.76 & > 0.70 \\
\hline
\end{tabular}
\caption{Résultats de validation}
\end{table}

\subsection{Validation Croisée}

Le modèle a été validé avec une validation croisée 5-fold, une technique statistique rigoureuse qui garantit une évaluation robuste et non biaisée de la performance du modèle. La validation croisée divise le dataset en 5 sous-ensembles (folds) de taille approximativement égale, puis entraîne le modèle 5 fois, en utilisant à chaque fois 4 folds pour l'entraînement et le fold restant pour la validation. Cette approche garantit que chaque observation du dataset est utilisée exactement une fois pour la validation, permettant ainsi une évaluation complète et non biaisée de la performance du modèle.

Le split des données suit une répartition de 80\% pour l'entraînement et 20\% pour le test, une proportion standard dans le Machine Learning qui équilibre le besoin de données suffisantes pour l'entraînement avec le besoin de données suffisantes pour une évaluation fiable. Cette répartition garantit que le modèle a accès à suffisamment de données pour apprendre les patterns complexes dans les données, tout en réservant une portion significative des données pour une évaluation indépendante qui reflète la performance réelle du modèle sur de nouvelles données non vues.

La stratification est appliquée pour préserver la distribution des classes dans chaque fold, garantissant que chaque fold contient une proportion similaire de patients à haut risque et de patients à faible risque. Cette préservation est cruciale en médecine, où les classes peuvent être déséquilibrées (moins de patients à haut risque que de patients à faible risque), car elle garantit que le modèle est évalué sur des distributions représentatives. Sans stratification, certains folds pourraient contenir une proportion anormalement élevée ou faible de patients à haut risque, biaisant ainsi l'évaluation de la performance du modèle.

Les métriques moyennes sont calculées sur les 5 folds, fournissant ainsi une estimation robuste de la performance du modèle qui est moins sensible aux variations aléatoires dans la composition des folds. Cette moyenne permet également de quantifier l'incertitude dans l'estimation de la performance : si les métriques varient significativement entre les folds, cela indique que le modèle peut être instable ou que le dataset peut avoir des sous-populations avec des caractéristiques différentes. Cette information est précieuse car elle permet d'identifier les limitations potentielles du modèle et de guider les efforts d'amélioration.

\section{Tests d'Intégration}

Les tests d'intégration vérifient que tous les microservices communiquent correctement entre eux et que le workflow complet fonctionne de bout en bout.

\subsection{Workflow Complet}

Le workflow complet a été automatisé dans le fichier \texttt{tests/integration/test\_workflow.py} et teste séquentiellement toutes les étapes du processus de prédiction de risque. Le test commence par la création d'un patient dans HAPI FHIR via ProxyFHIR, vérifiant que les données sont correctement transmises et stockées. La synchronisation vers PostgreSQL est ensuite testée, garantissant que ProxyFHIR récupère et persiste correctement les données FHIR. L'anonymisation du patient par le service DeID est validée, vérifiant que les identifiants sont correctement supprimés et qu'un pseudo-ID unique est généré. L'extraction de plus de 30 features par Featurizer est testée, incluant la validation des features NLP extraites depuis les notes cliniques. La prédiction du risque par ModelRisque est vérifiée, confirmant que le modèle XGBoost génère des scores de risque valides. La génération d'explications SHAP est testée, garantissant que les explications sont cohérentes avec les prédictions. Enfin, l'affichage dans le Frontend est validé via des tests Selenium, vérifiant que les données sont correctement présentées à l'utilisateur.

Le workflow complet automatisé teste séquentiellement toutes les étapes du processus de prédiction de risque, garantissant que l'ensemble du pipeline fonctionne de manière cohérente et fiable. La première étape consiste en la création d'un patient dans HAPI FHIR via ProxyFHIR, vérifiant que les données sont correctement transmises et stockées dans le serveur FHIR. Cette étape valide que ProxyFHIR peut communiquer efficacement avec HAPI FHIR et que les ressources FHIR sont correctement créées avec toutes les métadonnées nécessaires.

La synchronisation automatique vers PostgreSQL est ensuite testée, garantissant que ProxyFHIR récupère correctement les données depuis HAPI FHIR et les persiste dans PostgreSQL avec la structure appropriée. Cette étape valide que le processus de synchronisation fonctionne correctement, que les données sont correctement transformées du format FHIR vers le schéma relationnel de PostgreSQL, et que toutes les relations entre les entités sont correctement préservées.

L'anonymisation du patient selon HIPAA Safe Harbor est validée, vérifiant que tous les identifiants directs sont correctement supprimés ou transformés, et qu'un pseudo-ID unique est généré de manière déterministe. Cette étape est cruciale car elle garantit que le processus d'anonymisation fonctionne correctement et que les données anonymisées ne peuvent pas être utilisées pour ré-identifier les patients, même en combinant plusieurs sources d'information.

L'extraction de plus de 30 features par Featurizer est testée, incluant la validation des features NLP extraites depuis les notes cliniques. Cette étape valide que le service Featurizer peut correctement extraire toutes les features nécessaires depuis les données anonymisées, que les calculs sont corrects, et que les features NLP sont correctement extraites et quantifiées. Cette validation est particulièrement importante car les features NLP sont complexes et dépendent de modèles de langage sophistiqués qui peuvent avoir des comportements subtils.

La prédiction du risque avec XGBoost est vérifiée, confirmant que le modèle génère des scores de risque valides dans la plage attendue (0 à 1), que les niveaux de risque sont correctement catégorisés (FAIBLE, MOYEN, ÉLEVÉ), et que les intervalles de confiance sont calculés correctement. Cette étape valide que le modèle fonctionne correctement et que les prédictions sont cohérentes avec les données d'entrée.

La génération d'explications SHAP est testée, garantissant que les explications sont cohérentes avec les prédictions, que les valeurs SHAP sont calculées correctement, et que les facteurs de risque sont correctement identifiés et ordonnés par importance. Cette validation est essentielle car les explications SHAP sont complexes à calculer et doivent être cohérentes avec les prédictions pour être utiles aux cliniciens.

Enfin, l'affichage dans le Frontend est validé via des tests Selenium, vérifiant que les données sont correctement présentées à l'utilisateur, que les graphiques sont correctement générés, et que les interactions utilisateur fonctionnent comme prévu. Cette étape valide que l'ensemble du système, depuis la collecte des données jusqu'à la présentation des résultats, fonctionne de manière cohérente et offre une expérience utilisateur fluide.

\subsection{Couverture des Tests}

Les tests d'intégration couvrent exhaustivement toutes les interactions critiques entre services, garantissant que chaque point de communication fonctionne correctement et que les données sont correctement transformées et transmises à chaque étape. L'interaction entre ProxyFHIR et HAPI FHIR pour la synchronisation est testée en détail, validant que ProxyFHIR peut récupérer correctement les ressources FHIR depuis HAPI FHIR, que les données sont correctement parsées et validées, et que les erreurs de communication sont correctement gérées. Cette validation est essentielle car la synchronisation est le point d'entrée des données dans le système, et toute erreur à ce niveau se propagerait à travers tout le système.

L'interaction entre ProxyFHIR et PostgreSQL pour la persistance est testée, validant que les données FHIR sont correctement transformées du format JSON FHIR vers le schéma relationnel de PostgreSQL, que toutes les relations entre entités sont correctement préservées, et que les contraintes d'intégrité sont respectées. Cette validation garantit que les données sont stockées de manière cohérente et peuvent être récupérées efficacement par les autres services.

L'interaction entre DeID et PostgreSQL pour l'anonymisation est testée, validant que le service DeID peut correctement lire les données depuis PostgreSQL, appliquer les transformations d'anonymisation selon HIPAA Safe Harbor, et stocker les données anonymisées avec les pseudo-IDs générés. Cette validation est cruciale car l'anonymisation est un processus complexe qui doit être appliqué correctement pour garantir la confidentialité des données.

L'interaction entre Featurizer et DeID pour l'extraction depuis données anonymisées est testée, validant que Featurizer peut correctement accéder aux données anonymisées, extraire toutes les features nécessaires, et stocker les features calculées. Cette validation garantit que le pipeline d'extraction de features fonctionne correctement et que toutes les features sont correctement calculées depuis les données anonymisées.

L'interaction entre ModelRisque et Featurizer pour la prédiction depuis features est testée, validant que ModelRisque peut correctement récupérer les features depuis Featurizer, préparer les features pour le modèle XGBoost, générer des prédictions valides, et calculer les explications SHAP. Cette validation garantit que le pipeline de prédiction fonctionne correctement et que les prédictions sont cohérentes avec les features d'entrée.

L'interaction entre ScoreAPI et tous les services pour l'orchestration est testée, validant que ScoreAPI peut correctement coordonner les appels aux différents services, agréger les réponses, et gérer les erreurs de manière appropriée. Cette validation est essentielle car ScoreAPI est le point d'entrée principal pour les clients, et doit garantir une expérience utilisateur fluide même lorsque certains services rencontrent des problèmes.

Enfin, l'interaction entre le Frontend et ScoreAPI pour l'interface utilisateur est testée, validant que le Frontend peut correctement communiquer avec ScoreAPI, que les données sont correctement affichées, et que les interactions utilisateur fonctionnent comme prévu. Cette validation garantit que l'expérience utilisateur est fluide et que les données sont correctement présentées aux utilisateurs finaux.

\section{Tests de Performance}

Les tests de performance utilisent Locust pour simuler une charge utilisateur réaliste et identifier les limites de performance du système.

\subsection{Métriques Mesurées}

Les tests de performance mesurent plusieurs métriques critiques pour évaluer la capacité du système à gérer une charge de production. Le temps de réponse moyen et le temps de réponse au 95e percentile sont mesurés pour chaque endpoint, permettant d'identifier les opérations lentes. Le débit (throughput) mesure le nombre de requêtes par seconde que le système peut traiter, indiquant la capacité de traitement globale. Le taux d'erreur sous charge identifie les problèmes de stabilité lorsque le système est soumis à une charge élevée. Enfin, l'utilisation des ressources (CPU, mémoire) est surveillée pour identifier les goulots d'étranglement et planifier la scalabilité.

\subsection{Scénarios de Test}

Plusieurs scénarios de charge sont testés pour simuler différents niveaux d'utilisation. Le scénario de charge normale simule 50 utilisateurs simultanés effectuant des opérations typiques (consultation de dashboard, liste de patients, scores de risque). Le scénario de charge élevée teste 200 utilisateurs simultanés pour identifier les limites du système actuel. Le scénario de pic de charge simule 500 utilisateurs simultanés pour tester la résilience et la dégradation gracieuse. Enfin, le scénario de stress prolongé maintient une charge élevée pendant 30 minutes pour détecter les fuites mémoire et les problèmes de performance à long terme.

\newpage

% ============================================
% DÉPLOIEMENT
% ============================================
\chapter{Déploiement et Installation}

\section{Prérequis}

Le déploiement de HealthFlow-MS nécessite un environnement système qui peut supporter l'exécution simultanée de 8 microservices, une base de données PostgreSQL, et un serveur FHIR, chacun avec ses propres exigences en termes de ressources. \textbf{Docker 20.10 ou supérieur} est requis car tous les services sont conteneurisés pour garantir la portabilité et la reproductibilité du déploiement. Docker fournit l'isolation nécessaire pour que chaque service fonctionne dans son propre environnement, évitant ainsi les conflits de dépendances et simplifiant la gestion des versions. La version 20.10 ou supérieure est nécessaire car elle apporte des améliorations significatives en termes de performance, de sécurité, et de fonctionnalités qui sont exploitées par le système.

\textbf{Docker Compose 2.0 ou supérieur} est requis pour orchestrer le déploiement de tous les services de manière coordonnée. Docker Compose permet de définir l'ensemble de l'architecture dans un fichier de configuration unique, gérant automatiquement la création des réseaux, le démarrage des services dans le bon ordre, et la gestion des dépendances entre services. La version 2.0 apporte des améliorations significatives en termes de performance et de fonctionnalités, notamment le support des profiles qui permettent de déployer différents sous-ensembles de services selon les besoins.

Un minimum de \textbf{8 GB de RAM} est nécessaire pour faire fonctionner tous les services simultanément, avec 16 GB recommandés pour des performances optimales. Cette exigence est principalement due au service Featurizer qui charge les modèles NLP (BioBERT et spaCy) en mémoire, nécessitant environ 6 GB de RAM à lui seul. Les autres services sont relativement légers en termes de mémoire, mais leur combinaison nécessite encore plusieurs gigaoctets. La recommandation de 16 GB permet d'avoir une marge de sécurité pour gérer les pics de charge et éviter les problèmes de mémoire qui pourraient dégrader les performances ou causer des crashes.

\textbf{20 GB d'espace disque libre} sont nécessaires pour stocker les images Docker de tous les services, les données de la base de données, et les modèles ML. Les images Docker peuvent être volumineuses, particulièrement celle de Featurizer qui contient les modèles NLP (environ 6 GB). La base de données nécessite également de l'espace pour stocker les données FHIR, les patients anonymisés, les features extraites, et les prédictions. Cette exigence d'espace permet également d'avoir une marge pour la croissance des données au fil du temps.

Enfin, plusieurs \textbf{ports doivent être disponibles} pour que les services puissent communiquer : le port 5432 pour PostgreSQL, les ports 8081 à 8087 pour les différents microservices, et le port 8090 pour HAPI FHIR. Ces ports doivent être libres sur la machine hôte pour éviter les conflits avec d'autres applications. Si certains ports sont déjà utilisés, ils peuvent être modifiés dans le fichier docker-compose.yml, mais cela nécessite également de mettre à jour les configurations des services qui communiquent entre eux.

\section{Installation}

\subsection{Clonage du Repository}

Le code source du projet est disponible sur GitHub à l'adresse suivante : \url{https://github.com/OsamaMansouri/HealthFlowMS}

Pour cloner le repository et accéder au code source :

\begin{lstlisting}[language=bash, caption=Clonage du projet]
git clone https://github.com/OsamaMansouri/HealthFlowMS.git
cd HealthFlowMS
\end{lstlisting}

\subsection{Démarrage des Services}

\begin{lstlisting}[language=bash, caption=Démarrage avec Docker Compose]
docker-compose up -d --build
\end{lstlisting}

Cette commande orchestre un processus complexe de déploiement qui garantit que tous les services sont correctement construits, configurés, et démarrés dans le bon ordre. Premièrement, elle construit les images Docker pour tous les services, compilant le code source, installant les dépendances, et créant les images optimisées qui seront utilisées pour l'exécution. Ce processus de build peut prendre plusieurs minutes car il doit télécharger les dépendances, compiler le code, et créer les images, mais il n'est nécessaire qu'une seule fois ou lorsque le code change.

Deuxièmement, elle démarre PostgreSQL et initialise le schéma de base de données, créant toutes les tables, index, et contraintes nécessaires pour le fonctionnement du système. Cette initialisation est cruciale car elle garantit que la base de données est dans un état cohérent et prête à recevoir des données. Le schéma est créé à partir de scripts SQL qui définissent la structure complète de la base de données, garantissant ainsi que tous les services peuvent correctement stocker et récupérer leurs données.

Troisièmement, elle démarre le serveur HAPI FHIR, qui constitue la source de données médicales pour l'ensemble du système. Le serveur FHIR est démarré en premier car les autres services en dépendent pour récupérer les données médicales. Le serveur est configuré avec les paramètres appropriés pour garantir la compatibilité avec le standard FHIR R4 et permettre les requêtes cross-origin depuis les autres services.

Quatrièmement, elle démarre tous les microservices dans l'ordre approprié, en respectant les dépendances entre services. Les services sont démarrés de manière séquentielle pour garantir que chaque service peut correctement se connecter à ses dépendances (base de données, autres services) avant de commencer à traiter des requêtes. Cette approche garantit que le système démarre de manière cohérente et que tous les services sont prêts à fonctionner avant que le système ne soit considéré comme opérationnel.

Enfin, elle configure le réseau Docker interne qui permet aux services de communiquer entre eux de manière sécurisée et isolée. Ce réseau virtuel garantit que les services peuvent communiquer efficacement sans exposer les ports internes à l'extérieur du système, améliorant ainsi la sécurité. Le réseau est configuré avec des règles de routage appropriées qui garantissent que chaque service peut accéder uniquement aux services dont il a besoin, suivant ainsi le principe du moindre privilège.

\subsection{Initialisation de la Base de Données}

Après le démarrage des services, il est nécessaire d'initialiser la base de données en créant les tables et les utilisateurs par défaut. Cette étape est essentielle pour que le système fonctionne correctement.

\textbf{Création des tables:}

\begin{lstlisting}[language=bash, caption=Création des tables de la base de données]
docker exec healthflow-score-api python -c "
import sys
sys.path.insert(0, '/app')
from app.database import Base, engine
from app.models import User, ApiAuditLog, RiskPrediction, DeidPatient
Base.metadata.create_all(bind=engine)
print('Tables creees avec succes')
"
\end{lstlisting}

\textbf{Création des utilisateurs par défaut:}

\begin{lstlisting}[language=bash, caption=Création des utilisateurs par défaut]
docker exec healthflow-score-api python -c "
import sys
sys.path.insert(0, '/app')
from app.database import SessionLocal
from app.services import UserService

db = SessionLocal()
user_service = UserService(db)

users = [
    {'username': 'admin', 'password': 'admin123', 'role': 'admin', 'email': 'admin@healthflow.local', 'full_name': 'Administrator'},
    {'username': 'clinician', 'password': 'admin123', 'role': 'clinician', 'email': 'clinician@healthflow.local', 'full_name': 'Clinical User'},
    {'username': 'researcher', 'password': 'admin123', 'role': 'researcher', 'email': 'researcher@healthflow.local', 'full_name': 'Researcher'},
    {'username': 'auditor', 'password': 'admin123', 'role': 'auditor', 'email': 'auditor@healthflow.local', 'full_name': 'Auditor'}
]

for u in users:
    if not user_service.get_user_by_username(u['username']):
        user_service.create_user(u['username'], u['email'], u['password'], u['full_name'], u['role'])
        print(f'Utilisateur cree: {u[\"username\"]}')
    else:
        print(f'Utilisateur {u[\"username\"]} existe deja')
"
\end{lstlisting}

\subsection{Vérification}

Vérifier que tous les services sont démarrés:

\begin{lstlisting}[language=bash, caption=Vérification des services]
docker-compose ps
\end{lstlisting}

Vérifier les healthchecks:

\begin{lstlisting}[language=bash, caption=Healthchecks]
curl http://localhost:8081/health
curl http://localhost:8082/health
curl http://localhost:8083/health
curl http://localhost:8084/health
curl http://localhost:8085/health
curl http://localhost:8086/health
\end{lstlisting}

Le healthcheck de ScoreAPI devrait retourner \texttt{\{"status":"UP","database":"connected",...\}} si la base de données est correctement initialisée.

\section{Configuration}

\subsection{Variables d'Environnement}

Les principales variables d'environnement sont définies dans \texttt{docker-compose.yml}, permettant ainsi de configurer le système sans modifier le code source. Cette approche de configuration externalisée est essentielle pour la flexibilité et la sécurité, car elle permet d'adapter le système à différents environnements (développement, test, production) sans avoir à recompiler le code. La variable \texttt{DB\_PASSWORD} définit le mot de passe pour la connexion à PostgreSQL, un paramètre critique pour la sécurité qui doit être choisi avec soin et stocké de manière sécurisée. Ce mot de passe est utilisé par tous les services qui accèdent à la base de données, garantissant ainsi que seuls les services autorisés peuvent accéder aux données sensibles.

La variable \texttt{DATABASE\_URL} définit l'URL complète de connexion à la base de données, incluant le protocole (postgresql://), l'adresse du serveur, le port, le nom de la base de données, et les paramètres de connexion. Cette URL est utilisée par tous les services Python pour se connecter à PostgreSQL via SQLAlchemy, et par le service Java pour se connecter via JDBC. La configuration centralisée de cette URL garantit que tous les services utilisent la même base de données et évite les erreurs de configuration qui pourraient causer des problèmes de connectivité.

La variable \texttt{LOG\_LEVEL} contrôle le niveau de verbosité des logs générés par les services, permettant ainsi d'ajuster la quantité d'information enregistrée selon les besoins. En développement, un niveau DEBUG peut être utilisé pour obtenir des informations détaillées sur le fonctionnement interne des services, facilitant ainsi le débogage. En production, un niveau INFO ou WARNING est généralement préféré pour réduire le volume de logs tout en conservant les informations importantes. Cette flexibilité est précieuse car elle permet d'adapter le système aux besoins spécifiques de chaque environnement sans avoir à modifier le code.

\subsection{Ports}

\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Service} & \textbf{Port} \\
\hline
PostgreSQL & 5432 \\
HAPI FHIR & 8090 \\
ProxyFHIR & 8081 \\
DeID & 8082 \\
Featurizer & 8083 \\
ModelRisque & 8084 \\
ScoreAPI & 8085 \\
AuditFairness & 8086 \\
Frontend & 8087 \\
\hline
\end{tabular}
\caption{Mapping des ports}
\end{table}

\section{Accès aux Services}

Une fois tous les services démarrés, ils sont accessibles via leurs URLs respectives, chacune optimisée pour un usage spécifique. Le \textbf{Frontend} est accessible à l'adresse \url{http://localhost:8087} et constitue le point d'entrée principal pour tous les utilisateurs du système. Cette interface web moderne offre une expérience utilisateur complète avec authentification, dashboard, gestion des patients, et visualisation des prédictions. L'interface est conçue pour être intuitive même pour les utilisateurs non techniques, avec des workflows guidés qui facilitent l'utilisation du système.

La documentation interactive de \textbf{ScoreAPI} est accessible à l'adresse \url{http://localhost:8085/docs}, offrant une interface Swagger/OpenAPI complète qui permet d'explorer tous les endpoints disponibles, de comprendre les schémas de données, et même de tester les APIs directement depuis le navigateur. Cette documentation est générée automatiquement depuis le code source, garantissant ainsi qu'elle est toujours à jour et reflète fidèlement les fonctionnalités disponibles. Cette documentation est précieuse pour les développeurs qui doivent intégrer le système avec d'autres applications ou pour les utilisateurs avancés qui préfèrent interagir directement avec les APIs.

Le serveur \textbf{HAPI FHIR} est accessible à l'adresse \url{http://localhost:8090/fhir/metadata}, permettant ainsi d'accéder aux métadonnées du serveur FHIR et de vérifier sa conformité au standard FHIR R4. L'endpoint /fhir/metadata retourne une description complète des capacités du serveur, incluant les ressources supportées, les opérations disponibles, et les extensions personnalisées. Cet endpoint est essentiel pour l'interopérabilité car il permet aux clients FHIR de découvrir automatiquement les capacités du serveur et de s'adapter en conséquence.

Enfin, le dashboard \textbf{AuditFairness} est accessible à l'adresse \url{http://localhost:8086}, offrant une interface interactive pour visualiser les métriques de fairness et détecter les biais dans les prédictions du modèle. Ce dashboard utilise Dash pour créer des visualisations interactives qui permettent d'explorer les métriques de fairness sous différents angles, de filtrer par groupe démographique, et d'identifier les patterns de biais. Cette interface est essentielle pour les administrateurs et les analystes qui doivent surveiller la fairness du modèle et prendre des mesures correctives lorsque des biais sont détectés.

\section{Comptes par Défaut}

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{Username} & \textbf{Password} & \textbf{Rôle} \\
\hline
admin & admin123 & admin \\
clinician & admin123 & clinician \\
researcher & admin123 & researcher \\
auditor & admin123 & auditor \\
\hline
\end{tabular}
\caption{Comptes par défaut (à changer en production)}
\end{table}

\newpage

% ============================================
% RÉSULTATS ET PERFORMANCES
% ============================================
\chapter{Résultats et Performances}

\section{Performances du Modèle ML}

\subsection{Métriques Globales}

Le modèle XGBoost a atteint les performances suivantes:

\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Métrique} & \textbf{Valeur} \\
\hline
AUC-ROC & 0.82 \\
Précision & 0.78 \\
Rappel & 0.74 \\
F1-Score & 0.76 \\
Spécificité & 0.79 \\
\hline
\end{tabular}
\caption{Performances du modèle}
\end{table}

\subsection{Analyse par Groupe de Risque}

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{Niveau de Risque} & \textbf{Seuil} & \textbf{Précision} \\
\hline
Faible (LOW) & < 0.4 & 0.85 \\
Moyen (MEDIUM) & 0.4 - 0.7 & 0.72 \\
Élevé (HIGH) & > 0.7 & 0.81 \\
\hline
\end{tabular}
\caption{Précision par niveau de risque}
\end{table}

\section{Performances Système}

\subsection{Temps de Réponse}

\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Opération} & \textbf{Temps Moyen} \\
\hline
Création patient & < 500ms \\
Anonymisation & < 1s \\
Extraction features & 2-5s (avec NLP) \\
Prédiction risque & < 500ms \\
Génération SHAP & 1-2s \\
\hline
\end{tabular}
\caption{Temps de réponse des opérations}
\end{table}

\subsection{Throughput}

Le débit (throughput) mesure la capacité du système à traiter les requêtes et constitue un indicateur critique de la scalabilité et de la performance globale. \textbf{ScoreAPI} peut traiter plus de 100 requêtes par seconde, une performance remarquable qui démontre l'efficacité de l'architecture microservices et de l'optimisation des requêtes. Ce débit élevé est rendu possible par plusieurs facteurs : l'utilisation de FastAPI qui offre des performances exceptionnelles grâce à son architecture asynchrone, l'optimisation des requêtes à la base de données avec des index appropriés, et la mise en cache des réponses fréquemment demandées. Cette capacité de traitement élevée garantit que le système peut gérer une charge importante sans dégradation significative des performances, essentiel pour un système qui doit être accessible à de nombreux utilisateurs simultanés.

\textbf{ModelRisque} peut générer plus de 50 prédictions par seconde, une performance impressionnante compte tenu de la complexité des calculs impliqués. Chaque prédiction nécessite de récupérer les features du patient, de les préparer pour le modèle XGBoost, d'exécuter la prédiction, et de calculer les explications SHAP, toutes des opérations qui peuvent être coûteuses en termes de temps de calcul. Cette performance est rendue possible par l'optimisation du modèle XGBoost, l'utilisation de calculs vectorisés avec NumPy, et la mise en cache du modèle et de l'explainer SHAP en mémoire. Cette capacité de traitement garantit que les prédictions peuvent être générées rapidement même sous charge élevée, permettant ainsi aux cliniciens d'obtenir les scores de risque sans délai significatif.

\textbf{Featurizer} peut effectuer 10 à 20 extractions de features par seconde lorsque l'analyse NLP est incluse, une performance qui reflète la complexité des opérations effectuées. L'extraction de features avec NLP est particulièrement coûteuse car elle nécessite de charger et d'exécuter les modèles BioBERT et spaCy, qui sont des modèles de langage sophistiqués nécessitant des calculs intensifs. Chaque extraction nécessite de traiter les notes cliniques avec BioBERT pour générer les embeddings sémantiques, d'extraire les entités avec spaCy, et de calculer les scores de sentiment, d'urgence, et de complexité. Cette performance, bien que plus faible que les autres services, est acceptable car l'extraction de features n'est généralement effectuée qu'une seule fois par patient, et les features sont ensuite stockées dans la base de données pour réutilisation ultérieure. Pour améliorer les performances, le système pourrait implémenter un traitement par lots (batch processing) qui traiterait plusieurs patients simultanément, exploitant ainsi mieux les capacités de calcul parallèle des modèles NLP.

\section{Utilisation des Ressources}

\subsection{Taille des Images Docker}

\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Service} & \textbf{Taille Image} \\
\hline
PostgreSQL & ~200 MB \\
HAPI FHIR & ~500 MB \\
ProxyFHIR & 363 MB \\
DeID & 448 MB \\
Featurizer & 5.91 GB (modèles NLP: BioBERT, spaCy) \\
ModelRisque & 1.59 GB (modèle XGBoost, SHAP) \\
ScoreAPI & 443 MB \\
AuditFairness & 1.21 GB (Dash, EvidentlyAI) \\
Frontend & 53.5 MB \\
\hline
\textbf{Total} & \textbf{~10.5 GB} \\
\hline
\end{tabular}
\caption{Taille des images Docker (mesurées)}
\end{table}

\subsection{Mémoire RAM}

L'utilisation de la mémoire RAM varie considérablement selon les services, reflétant la complexité et les exigences spécifiques de chaque composant. \textbf{PostgreSQL} utilise environ 200 MB de RAM en fonctionnement normal, une utilisation relativement modeste qui reflète l'efficacité du moteur de base de données. Cette utilisation peut augmenter sous charge lorsque PostgreSQL charge des pages de données fréquemment accédées dans son cache partagé, mais reste généralement dans des limites raisonnables. PostgreSQL est conçu pour être efficace en mémoire, utilisant des structures de données optimisées et un système de gestion de mémoire sophistiqué qui minimise l'utilisation de RAM tout en maximisant les performances.

\textbf{HAPI FHIR} utilise environ 500 MB de RAM, une utilisation modérée qui reflète la complexité du serveur FHIR qui doit gérer les ressources FHIR, valider les données selon le standard FHIR, et gérer les opérations de recherche complexes. Le serveur FHIR charge en mémoire les définitions de ressources FHIR et les validations nécessaires, ce qui explique cette utilisation de mémoire. Cette utilisation est acceptable car le serveur FHIR est un composant central du système qui doit être performant pour garantir que les autres services peuvent accéder rapidement aux données médicales.

\textbf{Featurizer} est le service le plus gourmand en mémoire, utilisant environ 6 GB de RAM lorsque les modèles NLP sont chargés. Cette utilisation importante est principalement due aux modèles BioBERT et spaCy qui sont des modèles de langage sophistiqués avec des millions de paramètres. BioBERT, en particulier, est un modèle transformer avec environ 110 millions de paramètres qui nécessitent plusieurs gigaoctets de mémoire pour être chargés et exécutés. spaCy charge également des modèles pré-entraînés qui nécessitent de la mémoire supplémentaire. Cette utilisation importante de mémoire est justifiée par la valeur ajoutée que ces modèles apportent : ils permettent d'extraire des informations précieuses des notes cliniques qui ne seraient pas disponibles autrement, améliorant ainsi significativement la précision des prédictions.

\textbf{ModelRisque} utilise environ 1.5 GB de RAM, principalement pour charger le modèle XGBoost et l'explainer SHAP en mémoire. Le modèle XGBoost, bien qu'optimisé, nécessite de la mémoire pour stocker la structure des arbres de décision et les paramètres du modèle. L'explainer SHAP nécessite également de la mémoire pour stocker les données d'entraînement nécessaires au calcul des valeurs SHAP. Cette utilisation de mémoire est justifiée par la nécessité de maintenir le modèle et l'explainer en mémoire pour garantir des temps de réponse rapides : charger le modèle depuis le disque à chaque prédiction serait beaucoup trop lent pour un système en production.

\textbf{AuditFairness} utilise environ 1 GB de RAM, principalement pour Dash, EvidentlyAI, et les données nécessaires aux calculs de fairness. Dash charge l'application web en mémoire, tandis qu'EvidentlyAI charge les données nécessaires pour calculer les métriques de fairness. Cette utilisation est acceptable car le dashboard d'audit n'est généralement utilisé que par un nombre limité d'utilisateurs (administrateurs, analystes) et n'a pas besoin de gérer une charge élevée.

Les autres services (ProxyFHIR, DeID, ScoreAPI) utilisent chacun entre 100 et 500 MB de RAM, une utilisation modeste qui reflète leur nature de services relativement simples qui effectuent principalement des transformations de données et des opérations de base de données. Ces services sont conçus pour être légers et efficaces, minimisant ainsi l'utilisation des ressources tout en maximisant les performances.

Le \textbf{total recommandé} de 12 à 16 GB de RAM permet d'avoir une marge de sécurité pour gérer les pics de charge, les opérations coûteuses comme l'extraction de features avec NLP, et la croissance future du système. Cette recommandation garantit que le système peut fonctionner de manière stable même sous charge élevée, sans risque de manque de mémoire qui pourrait causer des dégradations de performance ou des crashes. Pour des environnements de production avec une charge importante, une allocation de 16 GB ou plus est recommandée pour garantir des performances optimales.

\newpage

% ============================================
% CONCLUSION
% ============================================
\chapter{Conclusion et Perspectives}

\section{Réalisations}

Le projet HealthFlow-MS a réussi à atteindre tous ses objectifs ambitieux, démontrant la faisabilité et la valeur d'une approche intégrée combinant architecture microservices, standards d'interopérabilité, intelligence artificielle, et sécurité des données. L'implémentation d'une architecture microservices modulaire et scalable constitue une réalisation majeure qui démontre la maîtrise des concepts avancés d'architecture logicielle. Cette architecture comprend 8 microservices indépendants qui communiquent via des APIs REST, chacun avec sa propre responsabilité et sa propre stack technologique optimisée. Cette modularité garantit que le système peut évoluer et s'adapter aux besoins changeants sans nécessiter de refonte complète, et permet à chaque service d'être développé, testé, et déployé indépendamment.

L'intégration des standards FHIR pour l'interopérabilité représente une réalisation technique significative qui garantit que le système peut s'intégrer avec les systèmes d'information hospitaliers existants sans nécessiter de modifications majeures. Cette conformité au standard FHIR R4 garantit que les données médicales peuvent être échangées de manière standardisée, facilitant ainsi l'adoption du système dans différents environnements hospitaliers. L'implémentation complète du standard FHIR, incluant le support des ressources principales (Patient, Encounter, Observation, Condition) et des opérations de recherche complexes, démontre une compréhension approfondie des exigences d'interopérabilité dans le domaine de la santé.

Le développement d'un modèle ML performant avec un AUC-ROC de 0.82 constitue une réalisation remarquable qui démontre la capacité à appliquer les techniques avancées de Machine Learning à un problème médical réel. Ce niveau de performance est compétitif avec les modèles de recherche académique et démontre que le système peut effectivement aider à identifier les patients à risque de réadmission. Le processus rigoureux d'ingénierie des features, d'entraînement, et de validation qui a conduit à cette performance démontre une approche méthodique et scientifique du développement de modèles ML.

L'implémentation de l'anonymisation selon HIPAA Safe Harbor garantit que le système respecte les normes les plus strictes de protection des données de santé. Cette implémentation va bien au-delà d'une simple suppression de noms : elle applique méticuleusement toutes les exigences du standard HIPAA Safe Harbor, transformant les données identifiables en données anonymisées qui ne peuvent pas être utilisées pour ré-identifier les patients. Cette conformité est essentielle pour permettre l'utilisation du système dans des environnements de production réels où la protection des données est une exigence légale et éthique absolue.

La fourniture de l'explicabilité avec SHAP représente une innovation importante qui répond à une exigence critique en médecine : la nécessité de comprendre les décisions prises par les systèmes d'intelligence artificielle. Contrairement aux modèles "boîte noire" qui produisent des prédictions sans justification, HealthFlow-MS fournit des explications détaillées qui identifient les facteurs contribuant le plus au score de risque calculé. Cette transparence est essentielle pour la confiance clinique et permet aux médecins de prendre des décisions éclairées basées sur une compréhension complète du raisonnement du modèle.

La détection des biais avec EvidentlyAI garantit que le système ne perpétue pas ou n'amplifie pas les inégalités existantes dans les soins de santé. Le système calcule plusieurs métriques de fairness et génère des alertes lorsque des biais sont détectés, permettant ainsi une intervention proactive pour corriger les déséquilibres. Cette fonctionnalité démontre une compréhension profonde des enjeux éthiques de l'utilisation de l'intelligence artificielle en santé et un engagement à garantir l'équité dans les prédictions.

Enfin, la création d'une interface utilisateur moderne et intuitive garantit que les bénéfices du système sont accessibles à tous les professionnels de santé, indépendamment de leur niveau technique. L'interface web, développée avec React et TypeScript, offre une expérience utilisateur fluide avec des workflows guidés qui facilitent l'utilisation du système. Cette attention à l'expérience utilisateur est essentielle pour garantir l'adoption large du système et maximiser son impact sur la qualité des soins.

\section{Points Forts}

HealthFlow-MS présente plusieurs points forts distinctifs qui le positionnent comme une solution complète et innovante pour la prédiction de réadmission hospitalière. L'\textbf{architecture} microservices modulaire et indépendante constitue un avantage majeur qui garantit la scalabilité, la maintenabilité, et l'évolutivité du système. Chaque microservice peut être développé, testé, et déployé indépendamment, permettant ainsi une évolution rapide et flexible du système. Cette architecture permet également une scalabilité horizontale où chaque service peut être mis à l'échelle indépendamment selon ses besoins spécifiques, optimisant ainsi l'utilisation des ressources. La modularité garantit également que les pannes d'un service n'affectent pas les autres services, améliorant ainsi la résilience globale du système.

La \textbf{conformité aux standards FHIR} pour l'interopérabilité garantit que le système peut s'intégrer facilement avec les systèmes d'information hospitaliers existants sans nécessiter de modifications majeures. Cette conformité est un avantage compétitif significatif car elle réduit les coûts d'intégration et facilite l'adoption du système dans différents environnements hospitaliers. Le support complet du standard FHIR R4, incluant toutes les ressources principales et les opérations de recherche complexes, démontre une compréhension approfondie des exigences d'interopérabilité dans le domaine de la santé.

L'\textbf{intelligence artificielle} avec un modèle ML performant (AUC-ROC 0.82) et explicabilité via SHAP constitue un point fort majeur qui distingue HealthFlow-MS des systèmes traditionnels de prédiction. Le modèle XGBoost atteint des performances compétitives avec les modèles de recherche académique, démontrant la capacité à appliquer efficacement les techniques avancées de Machine Learning à un problème médical réel. L'explicabilité via SHAP est particulièrement précieuse car elle permet aux cliniciens de comprendre non seulement quel est le niveau de risque, mais également pourquoi ce niveau de risque a été calculé, facilitant ainsi l'interprétation clinique et la prise de décision.

La \textbf{sécurité} avec anonymisation HIPAA Safe Harbor et authentification JWT garantit que le système respecte les normes les plus strictes de protection des données de santé. L'anonymisation selon HIPAA Safe Harbor va bien au-delà d'une simple suppression de noms : elle applique méticuleusement toutes les exigences du standard, transformant les données identifiables en données anonymisées qui ne peuvent pas être utilisées pour ré-identifier les patients. L'authentification JWT offre une sécurité moderne et scalable qui évite de maintenir des sessions serveur, simplifiant ainsi la scalabilité horizontale tout en garantissant que seuls les utilisateurs autorisés peuvent accéder aux données sensibles.

Enfin, l'\textbf{expérience utilisateur} avec une interface moderne et des workflows guidés garantit que le système est accessible à tous les professionnels de santé, indépendamment de leur niveau technique. L'interface web, développée avec React et TypeScript, offre une expérience utilisateur fluide et intuitive qui masque la complexité technique sous-jacente. Les workflows guidés facilitent l'utilisation du système même pour les nouveaux utilisateurs, réduisant ainsi la courbe d'apprentissage et maximisant l'adoption du système. Cette attention à l'expérience utilisateur est essentielle pour garantir que les bénéfices du système sont accessibles à tous et que le système est effectivement utilisé dans la pratique clinique.

\section{Limitations et Améliorations Futures}

\subsection{Limitations Actuelles}

HealthFlow-MS, bien qu'étant un système complet et fonctionnel, présente certaines limitations qui reflètent les contraintes du développement initial et les opportunités d'amélioration future. La limitation la plus significative concerne le fait que le modèle ML a été entraîné sur des données synthétiques plutôt que sur des données hospitalières réelles. Cette limitation est due aux contraintes réglementaires et éthiques strictes qui régissent l'accès aux données médicales réelles, nécessitant des approbations institutionnelles complexes et des accords de partage de données détaillés. Bien que le modèle synthétique démontre la faisabilité de l'approche et atteint des performances raisonnables (AUC-ROC 0.82), un modèle entraîné sur des données réelles pourrait potentiellement atteindre des performances encore meilleures en capturant les patterns réels et les subtilités des données médicales authentiques. Cette limitation est reconnue et des efforts sont en cours pour obtenir l'accès à des données réelles qui permettront d'améliorer significativement la performance du modèle.

Une autre limitation concerne l'absence de cache pour les prédictions, nécessitant un recalcul à chaque requête même pour des patients dont les données n'ont pas changé. Cette approche garantit que les prédictions sont toujours basées sur les données les plus récentes, mais elle peut être inefficace pour les patients qui sont consultés fréquemment sans que leurs données médicales ne changent. L'implémentation d'un système de cache intelligent qui invalide automatiquement les prédictions lorsque les données sous-jacentes changent pourrait améliorer significativement les performances tout en maintenant la cohérence des données. Ce cache pourrait être implémenté avec Redis, une base de données en mémoire qui offre des performances exceptionnelles pour ce type d'utilisation.

Le système manque actuellement de monitoring avancé avec Prometheus et Grafana, limitant ainsi la capacité à surveiller en temps réel la santé du système, à détecter les problèmes avant qu'ils n'affectent les utilisateurs, et à analyser les tendances de performance sur le long terme. Un système de monitoring complet permettrait de collecter des métriques détaillées sur les performances de chaque service, d'identifier les goulots d'étranglement, de détecter les anomalies, et de générer des alertes automatiques lorsque des problèmes sont détectés. Cette fonctionnalité est essentielle pour un système de production qui doit garantir une disponibilité élevée et des performances optimales.

Enfin, l'absence de load balancing pour la haute disponibilité limite la capacité du système à gérer les pannes de service et à distribuer la charge entre plusieurs instances. Un load balancer permettrait de distribuer les requêtes entre plusieurs instances de chaque service, améliorant ainsi la capacité de traitement et garantissant que le système reste disponible même si une instance d'un service tombe en panne. Cette fonctionnalité est particulièrement importante pour les services critiques comme ScoreAPI et ModelRisque qui doivent être disponibles en permanence pour garantir que les cliniciens peuvent accéder aux prédictions de risque à tout moment.

\subsection{Perspectives d'Amélioration}

Les perspectives d'amélioration future sont nombreuses et prometteuses, ouvrant la voie vers un système encore plus performant, scalable, et utile. L'\textbf{entraînement sur données réelles} en collectant des données hospitalières réelles constitue la priorité absolue pour améliorer la performance du modèle. Un modèle entraîné sur des données réelles pourrait capturer les patterns subtils et les nuances spécifiques aux données médicales authentiques, potentiellement améliorant significativement la précision des prédictions. Cette amélioration nécessiterait des partenariats avec des établissements hospitaliers, des approbations éthiques et réglementaires, et des accords de partage de données détaillés, mais l'impact potentiel sur la performance justifie ces efforts.

L'\textbf{optimisation du modèle} via le fine-tuning des hyperparamètres représente une opportunité d'amélioration continue qui pourrait améliorer progressivement la performance sans nécessiter de changements majeurs. Les techniques d'optimisation comme la recherche bayésienne d'hyperparamètres, l'early stopping adaptatif, et l'ensemble learning pourraient potentiellement améliorer l'AUC-ROC de quelques points de pourcentage supplémentaires. Cette optimisation continue est essentielle pour maintenir la compétitivité du modèle et garantir qu'il reste performant même lorsque les patterns dans les données évoluent.

L'implémentation d'un \textbf{cache avec Redis} pour mettre en cache les prédictions pourrait améliorer significativement les performances en évitant de recalculer les prédictions pour des patients dont les données n'ont pas changé. Redis, une base de données en mémoire, offre des performances exceptionnelles pour ce type d'utilisation, avec des temps de lecture de l'ordre de la microseconde. Un système de cache intelligent pourrait invalider automatiquement les prédictions lorsque les données sous-jacentes changent, garantissant ainsi la cohérence des données tout en améliorant les performances. Cette amélioration serait particulièrement bénéfique pour les scénarios où les mêmes patients sont consultés fréquemment, comme dans les unités de soins intensifs où les patients sont surveillés en continu.

L'intégration de \textbf{monitoring avec Prometheus et Grafana} permettrait de surveiller en temps réel la santé du système, de détecter les problèmes avant qu'ils n'affectent les utilisateurs, et d'analyser les tendances de performance sur le long terme. Prometheus collecterait les métriques détaillées de chaque service, tandis que Grafana fournirait des visualisations interactives qui permettraient d'identifier rapidement les problèmes et les opportunités d'optimisation. Ce système de monitoring serait essentiel pour un déploiement en production, permettant de garantir une disponibilité élevée et des performances optimales.

Un \textbf{pipeline CI/CD} pour le déploiement automatique améliorerait significativement la productivité du développement en automatisant les tests, la construction, et le déploiement. Ce pipeline garantirait que seuls les changements qui passent tous les tests sont déployés, réduisant ainsi le risque d'introduire des bugs en production. L'automatisation du déploiement réduirait également le temps nécessaire pour mettre à jour le système, permettant ainsi des améliorations plus fréquentes et une réponse plus rapide aux besoins des utilisateurs.

Le \textbf{scaling avec Kubernetes} pour la mise à l'échelle horizontale permettrait au système de gérer des charges beaucoup plus importantes en distribuant la charge entre plusieurs instances de chaque service. Kubernetes orchestrerait automatiquement le déploiement, la mise à l'échelle, et la gestion des services, garantissant que le système peut s'adapter dynamiquement à la charge. Cette capacité de scaling serait essentielle pour un déploiement à grande échelle dans des environnements hospitaliers avec de nombreux utilisateurs simultanés.

Le support \textbf{multi-tenant} pour plusieurs hôpitaux permettrait au système de servir plusieurs établissements hospitaliers simultanément, chacun avec ses propres données isolées et ses propres configurations. Cette fonctionnalité serait précieuse pour les déploiements à grande échelle où un seul système pourrait servir plusieurs hôpitaux, réduisant ainsi les coûts d'infrastructure et facilitant la maintenance. L'isolation des données garantirait que chaque hôpital ne peut accéder qu'à ses propres données, respectant ainsi les exigences de confidentialité et de conformité.

Enfin, une \textbf{API GraphQL} comme alternative à REST offrirait une flexibilité accrue pour les requêtes, permettant aux clients de spécifier exactement quelles données ils souhaitent récupérer, réduisant ainsi la surcharge réseau et améliorant les performances. GraphQL serait particulièrement utile pour les clients qui ont besoin d'accéder à des données complexes avec de nombreuses relations, comme les détails complets d'un patient avec son historique médical, ses features, et ses prédictions. Cette flexibilité améliorerait l'expérience développeur et permettrait une intégration plus facile avec d'autres systèmes.

\section{Impact Potentiel}

HealthFlow-MS peut avoir un impact significatif et transformateur sur la qualité des soins, l'efficacité opérationnelle, et les résultats pour les patients. La \textbf{réduction des réadmissions} grâce à l'identification précoce des patients à risque constitue l'impact le plus direct et le plus mesurable du système. En identifiant les patients à haut risque de réadmission avant leur sortie de l'hôpital, le système permet aux équipes de soins de mettre en place des interventions préventives ciblées qui peuvent réduire significativement le risque de réadmission. Ces interventions peuvent inclure des visites de suivi à domicile, des consultations avec des spécialistes, des ajustements de médication, ou des programmes d'éducation du patient. L'identification précoce est cruciale car elle permet d'intervenir avant que les problèmes ne s'aggravent, maximisant ainsi l'efficacité des interventions préventives.

L'\textbf{optimisation des ressources} via l'allocation ciblée des soins permet aux établissements hospitaliers d'utiliser leurs ressources limitées de manière plus efficace et efficiente. En identifiant les patients qui bénéficieraient le plus d'interventions préventives, le système permet de concentrer les ressources sur les cas où elles auront le plus d'impact. Cette allocation ciblée est particulièrement précieuse dans un contexte où les ressources de santé sont limitées et où il est essentiel de maximiser l'impact de chaque intervention. L'optimisation des ressources peut également améliorer l'efficacité opérationnelle en réduisant le temps et les efforts consacrés aux patients à faible risque, permettant ainsi aux équipes de soins de se concentrer sur les cas les plus complexes et les plus critiques.

L'\textbf{amélioration de la qualité} via une continuité des soins améliorée garantit que les patients reçoivent les soins appropriés au bon moment, réduisant ainsi les risques de complications et améliorant les résultats pour les patients. La continuité des soins est essentielle pour la qualité des soins, car elle garantit que les informations importantes sont partagées entre les différents professionnels de santé impliqués dans les soins d'un patient. Le système facilite cette continuité en fournissant un accès centralisé à toutes les informations pertinentes sur le risque de réadmission, permettant ainsi aux équipes de soins de prendre des décisions éclairées basées sur une compréhension complète de l'état du patient.

Enfin, la \textbf{réduction des coûts} via la prévention des réadmissions coûteuses représente un impact économique significatif qui peut justifier l'investissement dans le système. Les réadmissions non planifiées sont extrêmement coûteuses pour les systèmes de santé, représentant des milliards de dollars en coûts évitables chaque année. En réduisant le nombre de réadmissions, le système peut générer des économies substantielles qui peuvent être réinvesties dans d'autres aspects des soins de santé. Cette réduction des coûts est particulièrement précieuse dans un contexte où les budgets de santé sont contraints et où il est essentiel de maximiser la valeur de chaque dollar dépensé. L'impact économique, combiné avec l'impact sur la qualité des soins, fait de HealthFlow-MS un investissement précieux pour les établissements hospitaliers qui cherchent à améliorer à la fois les résultats pour les patients et l'efficacité opérationnelle.

\newpage

% ============================================
% ANNEXES
% ============================================
\appendix

\chapter{Commandes Utiles}

\section{Docker}

\begin{lstlisting}[language=bash, caption=Commandes Docker utiles]
# Démarrer tous les services
docker-compose up -d --build

# Arrêter tous les services
docker-compose down

# Voir les logs d'un service
docker logs healthflow-proxy-fhir
docker logs healthflow-deid
docker logs healthflow-featurizer

# Redémarrer un service
docker-compose restart proxy-fhir

# Voir l'utilisation des ressources
docker stats
\end{lstlisting}

\section{Base de Données}

\begin{lstlisting}[language=bash, caption=Commandes PostgreSQL]
# Se connecter à PostgreSQL
docker exec -it healthflow-postgres psql -U healthflow -d healthflow

# Voir les tables
\dt

# Compter les patients
SELECT COUNT(*) FROM fhir_patients;
SELECT COUNT(*) FROM deid_patients;

# Voir les prédictions récentes
SELECT * FROM risk_predictions 
ORDER BY prediction_timestamp DESC LIMIT 10;
\end{lstlisting}

\chapter{Informations sur l'Équipe}

\section{Auteurs du Projet}

Ce projet a été développé par le \textbf{Groupe 8} de l'\textbf{EMSI} (École Marocaine des Sciences de l'Ingénieur) dans le cadre d'un projet de fin d'études. Le développement a été réalisé en collaboration étroite entre les membres de l'équipe, chacun apportant son expertise dans différents domaines techniques.

\subsection{Membres de l'Équipe}

Le projet HealthFlow-MS a été développé par une équipe de trois étudiants du Groupe 8 de l'EMSI qui ont collaboré étroitement tout au long du développement pour créer un système complet et intégré. \textbf{Osama MANSOURI} (\texttt{mansouri.osama@gmail.com}), \textbf{Sohaib LAARICHI} (\texttt{Sohaiblaatichi112@gmail.com}), et \textbf{Abouelkemhe Salah Eddine} (\texttt{slaaaheddine@gmail.com}) ont travaillé ensemble sur tous les aspects du projet, partageant les responsabilités et collaborant sur l'architecture microservices, le développement backend et frontend, l'intégration FHIR, le Machine Learning, et le déploiement Docker. Cette approche collaborative a permis de bénéficier des compétences complémentaires de chaque membre tout en garantissant une compréhension partagée de l'ensemble du système.

\section{Encadrement}

Le projet a été encadré par trois professeurs de l'EMSI qui ont apporté leur expertise et leur guidance tout au long du développement. \textbf{Pr. Mohamed LACHGAR} a encadré l'équipe sur les aspects d'architecture microservices et de systèmes distribués, guidant la conception globale du système et les bonnes pratiques de développement. \textbf{Pr. Oumayma OUEDRHIRI} a encadré l'équipe sur les aspects de Machine Learning et d'intelligence artificielle, guidant le développement du modèle de prédiction XGBoost et l'optimisation des performances. \textbf{Pr. Hiba TABBAA} a encadré l'équipe sur les aspects de Data Mining et de traitement du langage naturel, guidant l'extraction de features et l'intégration des techniques NLP (BioBERT, spaCy) pour analyser les notes cliniques.

\section{Contexte Académique}

Ce projet s'inscrit dans le cadre du projet de fin d'études du \textbf{Groupe 8} à l'\textbf{EMSI} (École Marocaine des Sciences de l'Ingénieur). Il démontre l'intégration de compétences multiples en architecture logicielle, développement web, intelligence artificielle, et traitement de données médicales, appliquées à un problème réel de santé publique. Le projet illustre la capacité de l'équipe à concevoir, développer et déployer un système complexe intégrant plusieurs technologies modernes et répondant aux exigences de sécurité, performance et utilisabilité dans le domaine médical.

Le code source complet du projet, incluant tous les microservices, la documentation, et les scripts de déploiement, est disponible publiquement sur GitHub à l'adresse \url{https://github.com/OsamaMansouri/HealthFlowMS}. Ce repository contient l'ensemble du code source, les fichiers de configuration Docker, la documentation technique, et les instructions de déploiement, permettant ainsi à d'autres développeurs et chercheurs de reproduire le système, de contribuer à son amélioration, ou de l'adapter à leurs propres besoins.

\chapter{Structure des Fichiers}

\begin{lstlisting}[language=bash, caption=Structure du projet]
HealthFlowMS/
├── database/
│   └── init/
│       └── 01_schema.sql
├── proxy-fhir/
│   └── src/main/java/...
├── deid/
│   └── app/
├── featurizer/
│   └── app/
├── model-risque/
│   └── app/
├── score-api/
│   └── app/
├── audit-fairness/
│   └── app/
├── frontend/
│   └── src/
├── docker-compose.yml
└── README.md
\end{lstlisting}

\chapter{Références}

Ce projet s'appuie sur une riche collection de ressources techniques, académiques, et réglementaires qui ont guidé le développement et garantissent la conformité aux standards les plus élevés. La \textbf{spécification HL7 FHIR} (\url{https://www.hl7.org/fhir/}) constitue la référence fondamentale pour l'interopérabilité des données médicales, définissant le standard complet pour l'échange de ressources médicales. Cette spécification a guidé chaque aspect de l'intégration FHIR dans le système, garantissant que toutes les ressources sont correctement structurées selon le standard FHIR R4 et que toutes les opérations respectent les conventions définies par HL7. La conformité à cette spécification est essentielle pour garantir l'interopérabilité avec les systèmes d'information hospitaliers existants et faciliter l'adoption du système dans différents environnements.

La \textbf{documentation XGBoost} (\url{https://xgboost.readthedocs.io/}) a fourni les informations détaillées nécessaires pour comprendre et optimiser l'utilisation de l'algorithme XGBoost dans le contexte du projet. Cette documentation couvre tous les aspects de l'utilisation de XGBoost, depuis les concepts fondamentaux jusqu'aux techniques avancées d'optimisation, permettant ainsi de tirer pleinement parti des capacités de l'algorithme. Les exemples et les guides pratiques de la documentation ont été particulièrement utiles pour implémenter le modèle de manière efficace et pour optimiser les hyperparamètres pour maximiser les performances.

La \textbf{documentation SHAP} (\url{https://shap.readthedocs.io/}) a fourni les informations nécessaires pour comprendre et implémenter les explications SHAP qui rendent les prédictions du modèle compréhensibles. Cette documentation explique en détail la théorie sous-jacente de SHAP, les différents types d'explainers disponibles, et les meilleures pratiques pour calculer et interpréter les valeurs SHAP. Cette compréhension approfondie a été essentielle pour implémenter correctement les explications SHAP et garantir qu'elles fournissent des informations utiles et interprétables pour les cliniciens.

L'article scientifique sur \textbf{BioBERT} (\url{https://arxiv.org/abs/1901.08746}) a fourni les informations détaillées sur le modèle de langage pré-entraîné qui est utilisé pour analyser les notes cliniques. Cet article explique l'architecture du modèle, le processus d'entraînement sur des corpus biomédicaux, et les performances du modèle sur différentes tâches NLP biomédicales. Cette compréhension approfondie a été essentielle pour choisir BioBERT comme modèle de langage et pour comprendre comment l'utiliser efficacement pour extraire des informations des notes cliniques.

Les informations sur le \textbf{standard HIPAA Safe Harbor} (\url{https://www.hhs.gov/hipaa/}) ont fourni les détails complets sur les exigences réglementaires pour l'anonymisation des données de santé. Cette ressource a été essentielle pour garantir que le processus d'anonymisation implémenté dans le système respecte fidèlement toutes les exigences du standard, transformant ainsi les données identifiables en données anonymisées qui ne peuvent pas être utilisées pour ré-identifier les patients. La conformité à ce standard est essentielle pour permettre l'utilisation du système dans des environnements de production réels où la protection des données est une exigence légale absolue.

La \textbf{documentation Docker} (\url{https://docs.docker.com/}) a fourni les informations nécessaires pour conteneuriser efficacement tous les services du système et orchestrer leur déploiement avec Docker Compose. Cette documentation couvre tous les aspects de Docker, depuis les concepts fondamentaux jusqu'aux fonctionnalités avancées comme les réseaux Docker, les volumes, et l'optimisation des images. Les guides pratiques de la documentation ont été essentiels pour créer des Dockerfiles optimisés qui minimisent la taille des images tout en garantissant que toutes les dépendances sont correctement installées.

La \textbf{documentation FastAPI} (\url{https://fastapi.tiangolo.com/}) a fourni les informations nécessaires pour développer efficacement les services Python avec FastAPI. Cette documentation est remarquablement complète et bien organisée, couvrant tous les aspects du développement avec FastAPI depuis les concepts fondamentaux jusqu'aux fonctionnalités avancées. Les exemples de code et les guides pratiques de la documentation ont été essentiels pour implémenter rapidement les APIs REST avec validation automatique des données, génération automatique de documentation, et support de l'asynchronicité.

La \textbf{documentation Spring Boot} (\url{https://spring.io/projects/spring-boot}) a fourni les informations nécessaires pour développer efficacement le service ProxyFHIR avec Spring Boot. Cette documentation couvre tous les aspects du développement avec Spring Boot, depuis la configuration de base jusqu'aux fonctionnalités avancées comme la gestion des transactions, l'intégration avec les bases de données, et la sécurité. Les guides pratiques et les exemples de code de la documentation ont été particulièrement utiles pour implémenter rapidement les fonctionnalités nécessaires tout en respectant les meilleures pratiques du framework.

Enfin, le \textbf{repository GitHub du projet} (\url{https://github.com/OsamaMansouri/HealthFlowMS}) constitue la source principale pour accéder au code source complet, à la documentation technique, et aux instructions de déploiement. Ce repository public permet la collaboration, la reproduction des résultats, et la contribution à l'amélioration continue du système.

\newpage

% ============================================
% BIBLIOGRAPHIE
% ============================================
\begin{thebibliography}{99}

\bibitem{fhir}
HL7 International. \textit{FHIR Release 4 (R4)}. 2019.
\url{https://www.hl7.org/fhir/}

\bibitem{xgboost}
Chen, T., \& Guestrin, C. (2016). \textit{XGBoost: A Scalable Tree Boosting System}. 
Proceedings of the 22nd ACM SIGKDD International Conference.

\bibitem{shap}
Lundberg, S. M., \& Lee, S. I. (2017). \textit{A Unified Approach to Interpreting Model Predictions}. 
Advances in Neural Information Processing Systems.

\bibitem{biobert}
Lee, J., et al. (2020). \textit{BioBERT: a pre-trained biomedical language representation model}. 
Bioinformatics, 36(4), 1234-1240.

\bibitem{hipaa}
U.S. Department of Health and Human Services. \textit{HIPAA Privacy Rule}. 
\url{https://www.hhs.gov/hipaa/}

\bibitem{microservices}
Newman, S. (2015). \textit{Building Microservices}. O'Reilly Media.

\bibitem{docker}
Merkel, D. (2014). \textit{Docker: lightweight Linux containers for consistent development and deployment}. 
Linux journal, 2014(239), 2.

\bibitem{github}
HealthFlowMS Project Repository. \textit{HealthFlowMS - Hospital Readmission Prediction System}. 
\url{https://github.com/OsamaMansouri/HealthFlowMS}

\end{thebibliography}

\end{document}

